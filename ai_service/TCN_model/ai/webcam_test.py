import cv2
import torch
import numpy as np
import itertools
import sys

# ==============================
# 0. CRITICAL MEDIAPIPE FIX & SHADOWING CHECK
# ==============================
import mediapipe as mp

# Check where mediapipe is actually loading from
mp_path = getattr(mp, '__file__', '')
if 'site-packages' not in mp_path and 'dist-packages' not in mp_path:
    print("\n" + "!"*70)
    print("ðŸš¨ ERROR: PYTHON IS LOADING THE WRONG MEDIAPIPE! ðŸš¨")
    print(f"It is loading from: {mp_path}")
    print("You have a local file named 'mediapipe.py' or a folder named 'mediapipe'.")
    print("Python is trying to use THAT instead of the real library.")
    print("Please RENAME or DELETE that file/folder, then run this script again.")
    print("!"*70 + "\n")
    sys.exit(1)

# Safe loading for newer MediaPipe versions
try:
    # Try direct import first (works in many 0.10.x versions where mp.solutions fails)
    from mediapipe.solutions import holistic as mp_holistic
    from mediapipe.solutions import face_mesh as mp_face_mesh
    from mediapipe.solutions import drawing_utils as mp_drawing
    from mediapipe.solutions import drawing_styles as mp_drawing_styles
except (ImportError, AttributeError, ModuleNotFoundError):
    try:
        # Fallback for some specific Python builds
        from mediapipe.python.solutions import holistic as mp_holistic
        from mediapipe.python.solutions import face_mesh as mp_face_mesh
        from mediapipe.python.solutions import drawing_utils as mp_drawing
        from mediapipe.python.solutions import drawing_styles as mp_drawing_styles
    except (ImportError, AttributeError, ModuleNotFoundError):
        # If all fail, the API is completely removed in this specific wheel
        print("\n" + "!"*70)
        print("ðŸš¨ MEDIAPIPE VERSION ERROR ðŸš¨")
        print("Your current MediaPipe version no longer supports the legacy 'solutions' API")
        print("which is required to match your friend's exact model architecture.")
        print("\nPlease run this command in your terminal to install a compatible version:")
        print("    pip install mediapipe==0.10.14")
        print("!"*70 + "\n")
        sys.exit(1)

# Import your friend's exact preprocessing functions from our new utils file
from asl_utils import (
    config, 
    preprocess_sequence_global, 
    hybrid_frame_strategy
)
from train_model import TCN, FEATURE_DIM  # Import the model architecture

# ==============================
# 1. SETUP & LOAD MODEL
# ==============================
DEVICE = "cpu"
MODEL_PATH = "tcn_best_cpu.pth"       # Generated by train_model.py
LABEL_PATH = "label_encoder.npy"      # Generated by train_model.py

try:
    LABELS = np.load(LABEL_PATH, allow_pickle=True)
    num_classes = len(LABELS)
    model = TCN(num_classes).to(DEVICE)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    model.eval()
    print("Model loaded successfully!")
except Exception as e:
    print(f"Error loading model: {e}")
    print("Did you run train_model.py first to generate the .pth file?")
    exit()

# ==============================
# 2. SETUP MEDIAPIPE MODEL
# ==============================
FACEMESH_LIPS = set(itertools.chain(*mp_face_mesh.FACEMESH_LIPS))
FACEMESH_LEFT_EYEBROW = set(itertools.chain(*mp_face_mesh.FACEMESH_LEFT_EYEBROW))
FACEMESH_RIGHT_EYEBROW = set(itertools.chain(*mp_face_mesh.FACEMESH_RIGHT_EYEBROW))
RELEVANT_FACE_INDICES = list(FACEMESH_LIPS | FACEMESH_LEFT_EYEBROW | FACEMESH_RIGHT_EYEBROW)
RELEVANT_FACE_INDICES.sort()

holistic = mp_holistic.Holistic(
    static_image_mode=False,
    model_complexity=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# ==============================
# 3. WEBCAM LOOP (AUTO-SENSE)
# ==============================
cap = cv2.VideoCapture(0)
buffer = []
recording = False
current_prediction = "Waiting for hands..."

# Auto-sensing parameters
HAND_MISSING_THRESHOLD = 10  # Number of frames to wait before finishing recording
frames_since_hand_seen = 0

print("\n--- AUTO-SENSE MODE ---")
print("Show your hands to start recording.")
print("Remove hands to trigger prediction.")
print("Press 'Q' to Quit\n")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret: break

    frame = cv2.flip(frame, 1)
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    rgb.flags.writeable = False
    results = holistic.process(rgb)
    
    # Check if any hand is visible
    hand_visible = results.left_hand_landmarks or results.right_hand_landmarks
    
    if results.face_landmarks:
        mp_drawing.draw_landmarks(
            frame, results.face_landmarks, mp_face_mesh.FACEMESH_TESSELATION,
            landmark_drawing_spec=None,
            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()
        )
        
    if results.pose_landmarks:
        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
            
    if results.left_hand_landmarks:
        mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
    if results.right_hand_landmarks:
        mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

    # --- EXTRACT KEYPOINTS ---
    pose = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4)
    lh = np.array([[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)
    rh = np.array([[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)
    
    if results.face_landmarks:
        relevant = [results.face_landmarks.landmark[i] for i in RELEVANT_FACE_INDICES]
        face = np.array([[lm.x, lm.y, lm.z] for lm in relevant]).flatten()
    else:
        face = np.zeros(len(RELEVANT_FACE_INDICES) * 3)

    final_kp = np.concatenate([pose, face, lh, rh])

    # --- AUTO-RECORDING LOGIC ---
    if hand_visible:
        if not recording:
            print("Hand detected! Recording...")
            buffer = [] # Clear old data
            recording = True
        
        buffer.append(final_kp)
        frames_since_hand_seen = 0
        cv2.putText(frame, "RECORDING...", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
    
    elif recording:
        # Hands are gone, but we were recording. Start the countdown.
        frames_since_hand_seen += 1
        buffer.append(final_kp) # Keep recording during the grace period
        
        cv2.putText(frame, f"Finishing in {HAND_MISSING_THRESHOLD - frames_since_hand_seen}", (20, 40), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)

        if frames_since_hand_seen >= HAND_MISSING_THRESHOLD:
            # TRIGGER PREDICTION
            recording = False
            original_length = len(buffer)
            
            if original_length > 10: # Only predict if it wasn't just a glitch
                print(f"Processing {original_length} frames...")
                raw_sequence = np.array(buffer)
                cleaned = preprocess_sequence_global(raw_sequence)
                sequences, masks, metadata = hybrid_frame_strategy(cleaned, original_length)

                all_probs = []
                for seq, mask in zip(sequences, masks):
                    if seq.shape != (config.TARGET_FRAMES, config.FEATURE_DIM): continue
                    x = torch.from_numpy(seq).float().unsqueeze(0).transpose(1, 2).to(DEVICE)
                    m = torch.from_numpy(mask).float().unsqueeze(0).to(DEVICE)
                    with torch.no_grad():
                        logits = model(x, m)
                        all_probs.append(torch.softmax(logits, dim=1)[0].cpu().numpy())

                if len(all_probs) > 0:
                    mean_probs = np.mean(np.stack(all_probs), axis=0)
                    top_idx = np.argmax(mean_probs)
                    current_prediction = f"{LABELS[top_idx]} ({mean_probs[top_idx]*100:.1f}%)"
                else:
                    current_prediction = "Error"
            else:
                current_prediction = "Too short!"

    # UI Overlay
    cv2.rectangle(frame, (0, 0), (640, 40) if not recording else (0,0), (0,0,0), -1) # Dark header
    cv2.putText(frame, f"Last Sign: {current_prediction}", (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
    cv2.imshow("ASL Auto-Sense", frame)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
