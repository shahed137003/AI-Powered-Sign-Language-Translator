import cv2
import torch
import numpy as np
import itertools
import sys

# ==============================
# 0. CRITICAL MEDIAPIPE FIX & SHADOWING CHECK
# ==============================
import mediapipe as mp

# Check where mediapipe is actually loading from
mp_path = getattr(mp, '__file__', '')
if 'site-packages' not in mp_path and 'dist-packages' not in mp_path:
    print("\n" + "!"*70)
    print("ðŸš¨ ERROR: PYTHON IS LOADING THE WRONG MEDIAPIPE! ðŸš¨")
    print(f"It is loading from: {mp_path}")
    print("You have a local file named 'mediapipe.py' or a folder named 'mediapipe'.")
    print("Python is trying to use THAT instead of the real library.")
    print("Please RENAME or DELETE that file/folder, then run this script again.")
    print("!"*70 + "\n")
    sys.exit(1)

# Safe loading for newer MediaPipe versions
try:
    # Try direct import first (works in many 0.10.x versions where mp.solutions fails)
    from mediapipe.solutions import holistic as mp_holistic
    from mediapipe.solutions import face_mesh as mp_face_mesh
    from mediapipe.solutions import drawing_utils as mp_drawing
    from mediapipe.solutions import drawing_styles as mp_drawing_styles
except (ImportError, AttributeError, ModuleNotFoundError):
    try:
        # Fallback for some specific Python builds
        from mediapipe.python.solutions import holistic as mp_holistic
        from mediapipe.python.solutions import face_mesh as mp_face_mesh
        from mediapipe.python.solutions import drawing_utils as mp_drawing
        from mediapipe.python.solutions import drawing_styles as mp_drawing_styles
    except (ImportError, AttributeError, ModuleNotFoundError):
        # If all fail, the API is completely removed in this specific wheel
        print("\n" + "!"*70)
        print("ðŸš¨ MEDIAPIPE VERSION ERROR ðŸš¨")
        print("Your current MediaPipe version no longer supports the legacy 'solutions' API")
        print("which is required to match your friend's exact model architecture.")
        print("\nPlease run this command in your terminal to install a compatible version:")
        print("    pip install mediapipe==0.10.14")
        print("!"*70 + "\n")
        sys.exit(1)

# Import your friend's exact preprocessing functions from our new utils file
from asl_utils import (
    config, 
    preprocess_sequence_global, 
    hybrid_frame_strategy
)
from train_model import TCN, FEATURE_DIM  # Import the model architecture

# ==============================
# 1. SETUP & LOAD MODEL
# ==============================
DEVICE = "cpu"
MODEL_PATH = "tcn_best_cpu.pth"       # Generated by train_model.py
LABEL_PATH = "label_encoder.npy"      # Generated by train_model.py

try:
    LABELS = np.load(LABEL_PATH, allow_pickle=True)
    num_classes = len(LABELS)
    model = TCN(num_classes).to(DEVICE)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    model.eval()
    print("Model loaded successfully!")
except Exception as e:
    print(f"Error loading model: {e}")
    print("Did you run train_model.py first to generate the .pth file?")
    exit()

# ==============================
# 2. SETUP MEDIAPIPE MODEL
# ==============================
FACEMESH_LIPS = set(itertools.chain(*mp_face_mesh.FACEMESH_LIPS))
FACEMESH_LEFT_EYEBROW = set(itertools.chain(*mp_face_mesh.FACEMESH_LEFT_EYEBROW))
FACEMESH_RIGHT_EYEBROW = set(itertools.chain(*mp_face_mesh.FACEMESH_RIGHT_EYEBROW))
RELEVANT_FACE_INDICES = list(FACEMESH_LIPS | FACEMESH_LEFT_EYEBROW | FACEMESH_RIGHT_EYEBROW)
RELEVANT_FACE_INDICES.sort()

holistic = mp_holistic.Holistic(
    static_image_mode=False,
    model_complexity=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# ==============================
# 3. WEBCAM LOOP
# ==============================
cap = cv2.VideoCapture(0)
buffer = []
recording = False
current_prediction = "Waiting..."

print("\n--- INSTRUCTIONS ---")
print("Press 'R' to Start Recording a sign")
print("Press 'E' to End Recording and Predict")
print("Press 'Q' to Quit\n")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret: break

    # Flip horizontally for selfie-view
    frame = cv2.flip(frame, 1)
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    rgb.flags.writeable = False
    results = holistic.process(rgb)
    
    # --- VISUALIZE KEYPOINTS ON FRAME ---
    # Draw face mesh
    if results.face_landmarks:
        mp_drawing.draw_landmarks(
            frame,
            results.face_landmarks,
            mp_face_mesh.FACEMESH_TESSELATION,
            landmark_drawing_spec=None,
            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()
        )
    # Draw pose
    if results.pose_landmarks:
        mp_drawing.draw_landmarks(
            frame,
            results.pose_landmarks,
            mp_holistic.POSE_CONNECTIONS,
            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()
        )
    # Draw left hand
    if results.left_hand_landmarks:
        mp_drawing.draw_landmarks(
            frame,
            results.left_hand_landmarks,
            mp_holistic.HAND_CONNECTIONS,
            landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style()
        )
    # Draw right hand
    if results.right_hand_landmarks:
        mp_drawing.draw_landmarks(
            frame,
            results.right_hand_landmarks,
            mp_holistic.HAND_CONNECTIONS,
            landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style()
        )

    # --- Extract exact same landmarks for AI model ---
    pose = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4)
    lh = np.array([[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)
    rh = np.array([[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)
    
    if results.face_landmarks:
        relevant = [results.face_landmarks.landmark[i] for i in RELEVANT_FACE_INDICES]
        face = np.array([[lm.x, lm.y, lm.z] for lm in relevant]).flatten()
    else:
        face = np.zeros(len(RELEVANT_FACE_INDICES) * 3)

    final_kp = np.concatenate([pose, face, lh, rh])

    if recording:
        buffer.append(final_kp)
        cv2.putText(frame, "RECORDING...", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
    
    # Display Prediction
    cv2.putText(frame, f"Prediction: {current_prediction}", (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
    cv2.imshow("ASL Webcam", frame)
    
    key = cv2.waitKey(1) & 0xFF

    if key == ord('r'):
        buffer = []
        recording = True
        current_prediction = "Recording..."

    if key == ord('e'):
        recording = False
        original_length = len(buffer)
        
        if original_length < 5:
            current_prediction = "Too short!"
            continue

        print(f"\nProcessing {original_length} frames...")
        
        # 1. Exact Preprocessing from friend's code
        raw_sequence = np.array(buffer)
        cleaned = preprocess_sequence_global(raw_sequence)

        # 2. Padding/Truncation Strategy
        sequences, masks, metadata = hybrid_frame_strategy(cleaned, original_length)

        # 3. Model Inference
        all_probs = []
        for seq, mask in zip(sequences, masks):
            if seq.shape != (config.TARGET_FRAMES, config.FEATURE_DIM):
                continue

            x = torch.from_numpy(seq).float().unsqueeze(0).transpose(1, 2).to(DEVICE)
            m = torch.from_numpy(mask).float().unsqueeze(0).to(DEVICE)

            with torch.no_grad():
                logits = model(x, m)
                probs = torch.softmax(logits, dim=1)
                all_probs.append(probs[0].cpu().numpy())

        if len(all_probs) > 0:
            mean_probs = np.mean(np.stack(all_probs), axis=0)
            top_idx = np.argmax(mean_probs)
            confidence = mean_probs[top_idx]
            current_prediction = f"{LABELS[top_idx]} ({confidence*100:.1f}%)"
            print(f"Result: {current_prediction}")
        else:
            current_prediction = "Error predicting"

    if key == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()