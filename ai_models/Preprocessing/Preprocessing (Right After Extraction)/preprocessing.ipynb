{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5836f5cc",
   "metadata": {},
   "source": [
    "# Pose-based Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37a9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4b4109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found: 11980\n",
      "Loaded samples: 11980\n",
      "labels: 11980\n",
      "Number of labels: 11980\n",
      "First sample shape: (74, 438)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"landmarks_mapped/landmarks_mapped\"  \n",
    "\n",
    "# List all .npy files\n",
    "files = glob.glob(os.path.join(data_path, \"*.npy\"))\n",
    "\n",
    "print(\"Total files found:\", len(files))\n",
    "\n",
    "data = []\n",
    "labels =[]  # Using a set to avoid repeated labels\n",
    "\n",
    "for file in files:\n",
    "    # Load npy file\n",
    "    arr = np.load(file, allow_pickle=True)\n",
    "    data.append(arr)\n",
    "\n",
    "    filename = os.path.basename(file)      \n",
    "    label = filename.split(\"_\")[0]           \n",
    "    label = label.split(\".\")[0]              \n",
    "    labels.append(label)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loaded samples:\", len(data))\n",
    "print(\"labels:\", len(labels))\n",
    "print(\"Number of labels:\", len(labels))\n",
    "print(\"First sample shape:\", data[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc80592",
   "metadata": {},
   "source": [
    "## Preprocessing Stage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c777f0",
   "metadata": {},
   "source": [
    "### Anchor Based Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95679d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First video shapes:\n",
      "Pose: (74, 132)\n",
      "Face: (74, 180)\n",
      "Left hand: (74, 63)\n",
      "Right hand: (74, 63)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pose_videos = []\n",
    "face_videos = []\n",
    "lh_videos = []\n",
    "rh_videos = []\n",
    "\n",
    "for video in data:\n",
    "    # Split along the last dimension (feature_dim)\n",
    "    pose_videos.append(video[:, 0:132])\n",
    "    face_videos.append(video[:, 132:312])\n",
    "    lh_videos.append(video[:, 312:375])\n",
    "    rh_videos.append(video[:, 375:438])\n",
    "\n",
    "# Check shapes of the first video\n",
    "print(\"First video shapes:\")\n",
    "print(\"Pose:\", pose_videos[0].shape)\n",
    "print(\"Face:\", face_videos[0].shape)\n",
    "print(\"Left hand:\", lh_videos[0].shape)\n",
    "print(\"Right hand:\", rh_videos[0].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd7a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_keypoints_reconstruction(hand_videos):\n",
    "    \"\"\"\n",
    "    Reconstruct missing hand keypoints in a list of videos.\n",
    "    Supports frames of shape (num_keypoints,) or (num_keypoints, dims)\n",
    "    \"\"\"\n",
    "\n",
    "    reconstructed_videos = []\n",
    "\n",
    "    for video in hand_videos:\n",
    "        video = video.copy()  # avoid modifying original\n",
    "        num_frames = video.shape[0]\n",
    "\n",
    "        # Detect valid frames\n",
    "        if video.ndim == 2:\n",
    "            # (num_frames, num_keypoints)\n",
    "            valid_frames = video[np.any(video != 0, axis=1)]\n",
    "        else:\n",
    "            # (num_frames, num_keypoints, dims)\n",
    "            valid_frames = video[np.any(video != 0, axis=(1,2))]\n",
    "\n",
    "        if len(valid_frames) == 0:\n",
    "            reconstructed_videos.append(video)\n",
    "            continue\n",
    "\n",
    "        avg_shape = np.mean(valid_frames, axis=0)\n",
    "\n",
    "        # Initialize first & last frames if missing\n",
    "        if np.all(video[0] == 0):\n",
    "            video[0] = avg_shape\n",
    "        if np.all(video[-1] == 0):\n",
    "            video[-1] = avg_shape\n",
    "\n",
    "        # Reconstruct missing frames\n",
    "        for k in range(num_frames):\n",
    "            if np.all(video[k] == 0):\n",
    "                # previous valid frame\n",
    "                alpha = 1\n",
    "                while k - alpha >= 0 and np.all(video[k - alpha] == 0):\n",
    "                    alpha += 1\n",
    "                # next valid frame\n",
    "                beta = 1\n",
    "                while k + beta < num_frames and np.all(video[k + beta] == 0):\n",
    "                    beta += 1\n",
    "                # Apply reconstruction\n",
    "                if k - alpha >= 0 and k + beta < num_frames:\n",
    "                    prev_frame = video[k - alpha]\n",
    "                    next_frame = video[k + beta]\n",
    "                    video[k] = (beta * prev_frame + alpha * next_frame) / (alpha + beta)\n",
    "                elif k - alpha >= 0:\n",
    "                    video[k] = video[k - alpha]\n",
    "                elif k + beta < num_frames:\n",
    "                    video[k] = video[k + beta]\n",
    "                else:\n",
    "                    video[k] = avg_shape\n",
    "\n",
    "        reconstructed_videos.append(video)\n",
    "\n",
    "    return reconstructed_videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f2dcf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_reconstructed=hand_keypoints_reconstruction(lh_videos)\n",
    "rh_reconstructed=hand_keypoints_reconstruction(rh_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b28e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Anchor-based normalization for a single frame\n",
    "# -----------------------------\n",
    "def anchor_normalize(landmarks, anchor_index=0, scale_points=None):\n",
    "    landmarks = np.asarray(landmarks, dtype=np.float32)\n",
    "\n",
    "    # If frame has no detected keypoints → return zeros\n",
    "    if np.all(landmarks == 0):\n",
    "        return landmarks\n",
    "\n",
    "    # If anchor keypoint is missing → skip normalization\n",
    "    if np.all(landmarks[anchor_index] == 0):\n",
    "        return landmarks\n",
    "\n",
    "    anchor = landmarks[anchor_index]\n",
    "\n",
    "    # Shift keypoints so anchor = (0,0)\n",
    "    normalized = landmarks - anchor\n",
    "\n",
    "    # Optional scale normalization\n",
    "    if scale_points is not None:\n",
    "        p1, p2 = scale_points\n",
    "        if np.all(normalized[p1] == 0) or np.all(normalized[p2] == 0):\n",
    "            return normalized\n",
    "\n",
    "        scale = np.linalg.norm(normalized[p1] - normalized[p2])\n",
    "        if scale > 1e-6:   # avoid division by zero\n",
    "            normalized /= scale\n",
    "\n",
    "    return normalized\n",
    "\n",
    "# -----------------------------\n",
    "# Normalize a single video (list or array of frames)\n",
    "# -----------------------------\n",
    "def normalize_video_list(video_frames, anchor_index=0, scale_points=None):\n",
    "    \"\"\"\n",
    "    video_frames: array (num_frames, num_points, dims) or list\n",
    "    Returns: list of normalized frames\n",
    "    \"\"\"\n",
    "    normalized_frames = []\n",
    "    for frame in video_frames:\n",
    "        normalized_frames.append(anchor_normalize(frame, anchor_index, scale_points))\n",
    "    return normalized_frames\n",
    "\n",
    "# -----------------------------\n",
    "# Normalize all streams (list of videos)\n",
    "# -----------------------------\n",
    "def normalize_all_streams_list(pose_list, lh_list, rh_list, face_list):\n",
    "    \"\"\"\n",
    "    pose_list, lh_list, rh_list, face_list: lists of videos (num_frames_i, num_points, dims)\n",
    "    Returns normalized lists for each stream\n",
    "    \"\"\"\n",
    "    pose_norm_list = [normalize_video_list(v, anchor_index=0, scale_points=(0, 1)) for v in pose_list]\n",
    "    lh_norm_list   = [normalize_video_list(v, anchor_index=0) for v in lh_list]\n",
    "    rh_norm_list   = [normalize_video_list(v, anchor_index=0) for v in rh_list]\n",
    "    face_norm_list = [normalize_video_list(v, anchor_index=0) for v in face_list]\n",
    "\n",
    "    return pose_norm_list, lh_norm_list, rh_norm_list, face_norm_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e848c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_norm, lh_norm, rh_norm, face_norm = normalize_all_streams_list(pose_videos, lh_reconstructed, rh_reconstructed, face_videos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1b602",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a71b69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def augment_keypoints_list(videos):\n",
    "    \"\"\"\n",
    "    Apply data augmentation: random rotation + Gaussian noise.\n",
    "    Accepts a list of videos OR a NumPy array (num_videos, num_frames, num_points, dims)\n",
    "    Returns a list of augmented videos.\n",
    "    \"\"\"\n",
    "    augmented_videos = []\n",
    "\n",
    "    # If input is a NumPy array (num_videos, num_frames, num_points, dims), convert to list\n",
    "    if isinstance(videos, np.ndarray) and videos.ndim == 4:\n",
    "        videos = [videos[i] for i in range(videos.shape[0])]\n",
    "\n",
    "    for keypoints in videos:\n",
    "        # Ensure keypoints is a NumPy array\n",
    "        keypoints = np.asarray(keypoints, dtype=np.float32)\n",
    "        keypoints_aug = keypoints.copy()\n",
    "\n",
    "        # 1. Random rotation (x-y plane)\n",
    "        angle_deg = np.random.uniform(-13, 13)\n",
    "        angle = np.deg2rad(angle_deg)\n",
    "        cos, sin = np.cos(angle), np.sin(angle)\n",
    "        rotation_matrix = np.array([[cos, -sin],\n",
    "                                    [sin,  cos]])\n",
    "        if keypoints_aug.shape[-1] >= 2:\n",
    "            keypoints_aug[..., :2] = keypoints_aug[..., :2] @ rotation_matrix.T\n",
    "\n",
    "        # 2. Add Gaussian noise\n",
    "        noise = np.random.normal(0, 1e-3, keypoints_aug.shape)\n",
    "        keypoints_aug += noise\n",
    "\n",
    "        # Store augmented video\n",
    "        augmented_videos.append(keypoints_aug)\n",
    "\n",
    "    return augmented_videos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0011001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pose_aug = augment_keypoints_list(pose_norm)\n",
    "lh_aug   = augment_keypoints_list(lh_norm)\n",
    "rh_aug   = augment_keypoints_list(rh_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a8a336",
   "metadata": {},
   "source": [
    "### prepare for feeding to the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6f27740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum frames: 233\n"
     ]
    }
   ],
   "source": [
    "all_videos = pose_aug + lh_aug + rh_aug + face_norm  # just to compute max frames\n",
    "max_frames = max(len(video) for video in all_videos)\n",
    "\n",
    "print(\"Maximum frames:\", max_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cdbf244",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d77a86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_videos_list(videos, max_frames):\n",
    "    \"\"\"\n",
    "    Pad a list of videos to the same number of frames.\n",
    "    videos: list of np.arrays, each (num_frames, num_points, dims) OR (num_frames, feature_dim)\n",
    "    max_frames: int, target number of frames\n",
    "    Returns: np.array of shape (num_videos, max_frames, *video_shape[1:])\n",
    "    \"\"\"\n",
    "    padded_videos = []\n",
    "    \n",
    "    for video in videos:\n",
    "        video = np.asarray(video, dtype=np.float32)\n",
    "        num_frames = video.shape[0]\n",
    "        pad_amount = max_frames - num_frames\n",
    "        \n",
    "        if pad_amount > 0:\n",
    "            # Create zeros to pad\n",
    "            if video.ndim == 3:  # (num_frames, num_points, dims)\n",
    "                pad_shape = (pad_amount, video.shape[1], video.shape[2])\n",
    "            elif video.ndim == 2:  # (num_frames, feature_dim)\n",
    "                pad_shape = (pad_amount, video.shape[1])\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported video shape: {video.shape}\")\n",
    "            \n",
    "            video_padded = np.vstack([video, np.zeros(pad_shape, dtype=np.float32)])\n",
    "        else:\n",
    "            video_padded = video\n",
    "        \n",
    "        padded_videos.append(video_padded)\n",
    "    \n",
    "    return np.array(padded_videos, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91277621",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_input = pad_videos_list(pose_aug, max_frames)\n",
    "lh_input   = pad_videos_list(lh_aug, max_frames)\n",
    "rh_input   = pad_videos_list(rh_aug, max_frames)\n",
    "face_input = pad_videos_list(face_norm, max_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3967334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11980, 233, 132)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_input.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "147f81f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final input shape: (11980, 233, 438)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate([pose_input, lh_input, rh_input,face_input], axis=-1)\n",
    "print(\"Final input shape:\", X.shape)  # (num_videos, max_frames, total_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66cf50d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shape: (11980,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)  # convert string labels to integers\n",
    "print(\"Label shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "128c9c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9584, 233, 438) (9584,)\n",
      "Test shape: (2396, 233, 438) (2396,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert your labels to integers if not already\n",
    "# Example: y = np.array([0, 1, 2, 0, 1, ...])\n",
    "\n",
    "# Split data: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6078f5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Save features\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "\n",
    "# Save labels\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "print(\"Data saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signlanguage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
