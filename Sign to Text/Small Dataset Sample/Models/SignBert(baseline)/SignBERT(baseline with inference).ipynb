{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd3ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import mediapipe as mp \n",
    "import cv2 as cv\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b119ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(r\"Preprocessed_No_Sliding_Window_OR_Mask\\Preprocessed_No_Sliding_Window_OR_Mask\")\n",
    "\n",
    "print(len(list(DATA_DIR.glob(\"*.npy\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "DATA_DIR = Path(r\"Preprocessed_No_Sliding_Window_OR_Mask\\Preprocessed_No_Sliding_Window_OR_Mask\")\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "TARGET_FRAMES = 157\n",
    "FEATURE_DIM = 438\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 90\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "PATIENCE = 12\n",
    "GRAD_CLIP = 1.0\n",
    "LABEL_SMOOTH = 0.1\n",
    "\n",
    "MODEL_SAVE_PATH = DATA_DIR / \"tcn_best_cpu_3rdcode.pth\"\n",
    "LABEL_ENCODER_PATH = DATA_DIR / \"label_encoder_3rdcode.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba44134",
   "metadata": {},
   "outputs": [],
   "source": [
    "files, masks, labels = [], [], []\n",
    "\n",
    "for f in DATA_DIR.glob(\"*.npy\"):\n",
    "    # print(\"HH\")\n",
    "\n",
    "    mask_f = f.with_name(f.stem + \".npy\")\n",
    "    # print(mask_f.exists())\n",
    "    if not mask_f.exists():\n",
    "        continue\n",
    "\n",
    "    arr = np.load(f)\n",
    "    if arr.shape != (TARGET_FRAMES, FEATURE_DIM):\n",
    "        continue\n",
    "\n",
    "    files.append(str(f))\n",
    "    masks.append(str(mask_f))\n",
    "    labels.append(f.stem.split(\"_\")[0])\n",
    "\n",
    "# Filter rare classes\n",
    "cnt = Counter(labels)\n",
    "keep = [i for i, y in enumerate(labels) if cnt[y] >= 2]\n",
    "\n",
    "files = [files[i] for i in keep]\n",
    "masks = [masks[i] for i in keep]\n",
    "labels = [labels[i] for i in keep]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "np.save(LABEL_ENCODER_PATH, le.classes_)\n",
    "num_classes = len(le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d835f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'about',\n",
       " 'after',\n",
       " 'angry',\n",
       " 'apple',\n",
       " 'aunt',\n",
       " 'baby',\n",
       " 'bad',\n",
       " 'bathroom',\n",
       " 'before',\n",
       " 'big',\n",
       " 'bird',\n",
       " 'blue',\n",
       " 'boy',\n",
       " 'brother',\n",
       " 'brown',\n",
       " 'brush',\n",
       " 'bug',\n",
       " 'can',\n",
       " 'candy',\n",
       " 'cannot',\n",
       " 'car',\n",
       " 'cat',\n",
       " 'cereal',\n",
       " 'cheese',\n",
       " 'child',\n",
       " 'church',\n",
       " 'clean',\n",
       " 'close',\n",
       " 'cold',\n",
       " 'come',\n",
       " 'cookie',\n",
       " 'cost',\n",
       " 'cow',\n",
       " 'cry',\n",
       " 'cup',\n",
       " 'dark',\n",
       " 'day',\n",
       " 'divorce',\n",
       " 'dog',\n",
       " 'down',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'eat',\n",
       " 'egg',\n",
       " 'excuse',\n",
       " 'father',\n",
       " 'finish',\n",
       " 'fork',\n",
       " 'friend',\n",
       " 'full',\n",
       " 'girl',\n",
       " 'go',\n",
       " 'gold',\n",
       " 'good',\n",
       " 'grandfather',\n",
       " 'grandmother',\n",
       " 'green',\n",
       " 'hamburger',\n",
       " 'happy',\n",
       " 'hear',\n",
       " 'help',\n",
       " 'here',\n",
       " 'holiday',\n",
       " 'home',\n",
       " 'homework',\n",
       " 'horse',\n",
       " 'hot',\n",
       " 'hotdog',\n",
       " 'how',\n",
       " 'hungry',\n",
       " 'hurt',\n",
       " 'in',\n",
       " 'less',\n",
       " 'light',\n",
       " 'like',\n",
       " 'love',\n",
       " 'milk',\n",
       " 'month',\n",
       " 'more',\n",
       " 'mother',\n",
       " 'need',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'no',\n",
       " 'not',\n",
       " 'now',\n",
       " 'off',\n",
       " 'open',\n",
       " 'orange',\n",
       " 'out',\n",
       " 'pants',\n",
       " 'pig',\n",
       " 'pizza',\n",
       " 'play',\n",
       " 'please',\n",
       " 'red',\n",
       " 'run',\n",
       " 'sad',\n",
       " 'same',\n",
       " 'school',\n",
       " 'see',\n",
       " 'sheep',\n",
       " 'shirt',\n",
       " 'shoes',\n",
       " 'silver',\n",
       " 'single',\n",
       " 'sister',\n",
       " 'sit',\n",
       " 'sleep',\n",
       " 'socks',\n",
       " 'sorry',\n",
       " 'spoon',\n",
       " 'stand',\n",
       " 'stop',\n",
       " 'tall',\n",
       " 'teacher',\n",
       " 'there',\n",
       " 'they',\n",
       " 'today',\n",
       " 'toothbrush',\n",
       " 'uncle',\n",
       " 'understand',\n",
       " 'underwear',\n",
       " 'up',\n",
       " 'wait',\n",
       " 'walk',\n",
       " 'want',\n",
       " 'wash',\n",
       " 'water',\n",
       " 'week',\n",
       " 'welcome',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'who',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'without',\n",
       " 'work',\n",
       " 'year',\n",
       " 'yellow',\n",
       " 'yes',\n",
       " 'yesterday',\n",
       " 'you',\n",
       " 'your'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f082672f",
   "metadata": {},
   "source": [
    "### After preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5067e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_tmp, y_tr, y_tmp, m_tr, m_tmp = train_test_split(\n",
    "    files, y, masks, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "X_val, X_te, y_val, y_te, m_val, m_te = train_test_split(\n",
    "    X_tmp, y_tmp, m_tmp, test_size=0.5, stratify=y_tmp, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4265e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4454\n",
      "[[-5.3782193e-03 -6.5089405e-01 -1.6726000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [-1.9704890e-03 -6.5519047e-01 -1.6631397e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 1.4372411e-03 -6.5948683e-01 -1.6536793e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " ...\n",
      " [ 6.9793008e-02 -6.8986171e-01 -1.1281021e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 6.9115520e-02 -6.8984360e-01 -1.1260076e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 6.8438031e-02 -6.8982548e-01 -1.1239130e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "X_tr_data = [np.load(Path(f)) for f in X_tr]\n",
    "\n",
    "print(len(X_tr_data))\n",
    "print(X_tr_data[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e8f0c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557\n",
      "(157, 438)\n"
     ]
    }
   ],
   "source": [
    "X_te_data = [np.load(Path(f)) for f in X_te]\n",
    "\n",
    "print(len(X_te_data))\n",
    "print(X_te_data[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e882cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557\n",
      "(157, 438)\n"
     ]
    }
   ],
   "source": [
    "X_val_data = [np.load(Path(f)) for f in X_val]\n",
    "\n",
    "print(len(X_val_data))\n",
    "print(X_val_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "04bce7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4454,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01910690",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_data=np.array(X_val_data)\n",
    "X_te_data=np.array(X_te_data)\n",
    "X_tr_data=np.array(X_tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "399f52e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4454, 157, 438)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f82e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(d_model)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / d_model)\n",
    "        angle_rads = pos * angle_rates\n",
    "\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        self.pos_encoding = tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65cff772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout, LayerNormalization\n",
    "\n",
    "class TransformerEncoderBlock(Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        attn_output = self.att(x, x)\n",
    "        x = self.norm1(x + self.dropout1(attn_output, training=training))\n",
    "        ffn_output = self.ffn(x)\n",
    "        return self.norm2(x + self.dropout2(ffn_output, training=training))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d4c878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_signbert_encoder(\n",
    "   T=157,\n",
    "    D=438,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    ff_dim=512,\n",
    "    num_layers=4\n",
    "):\n",
    "    inputs = Input(shape=(T, D))\n",
    "\n",
    "    # Pose embedding\n",
    "    x = Dense(d_model)(inputs)\n",
    "\n",
    "    # Positional encoding\n",
    "    x = PositionalEncoding(T, d_model)(x)\n",
    "\n",
    "    # Transformer encoder stack\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerEncoderBlock(d_model, num_heads, ff_dim)(x)\n",
    "\n",
    "    return Model(inputs, x, name=\"SignBERT_Encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b05dfc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_signbert_word_model(\n",
    "    T=157,\n",
    "    D=438,\n",
    "    num_classes=132\n",
    "):\n",
    "    encoder = build_signbert_encoder(T, D)\n",
    "\n",
    "    inputs = encoder.input\n",
    "    x = encoder.output\n",
    "\n",
    "    # Pool over time\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Classification head\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ce0ac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 157, 438)]        0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 157, 256)          112384    \n",
      "                                                                 \n",
      " positional_encoding (Posit  (None, 157, 256)          0         \n",
      " ionalEncoding)                                                  \n",
      "                                                                 \n",
      " transformer_encoder_block   (None, 157, 256)          2367488   \n",
      " (TransformerEncoderBlock)                                       \n",
      "                                                                 \n",
      " transformer_encoder_block_  (None, 157, 256)          2367488   \n",
      " 1 (TransformerEncoderBlock                                      \n",
      " )                                                               \n",
      "                                                                 \n",
      " transformer_encoder_block_  (None, 157, 256)          2367488   \n",
      " 2 (TransformerEncoderBlock                                      \n",
      " )                                                               \n",
      "                                                                 \n",
      " transformer_encoder_block_  (None, 157, 256)          2367488   \n",
      " 3 (TransformerEncoderBlock                                      \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 256)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 146)               37522     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9685650 (36.95 MB)\n",
      "Trainable params: 9685650 (36.95 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_signbert_word_model(\n",
    "   \n",
    "    num_classes=146\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32ad0d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "140/140 [==============================] - 674s 5s/step - loss: 5.0014 - accuracy: 0.0123 - val_loss: 4.9485 - val_accuracy: 0.0233\n",
      "Epoch 2/50\n",
      "140/140 [==============================] - 782s 6s/step - loss: 4.8772 - accuracy: 0.0225 - val_loss: 4.7207 - val_accuracy: 0.0413\n",
      "Epoch 3/50\n",
      "140/140 [==============================] - 680s 5s/step - loss: 4.6487 - accuracy: 0.0355 - val_loss: 4.4119 - val_accuracy: 0.0575\n",
      "Epoch 4/50\n",
      "140/140 [==============================] - 627s 4s/step - loss: 4.3033 - accuracy: 0.0687 - val_loss: 3.8825 - val_accuracy: 0.1059\n",
      "Epoch 5/50\n",
      "140/140 [==============================] - 692s 5s/step - loss: 3.9024 - accuracy: 0.0972 - val_loss: 3.5181 - val_accuracy: 0.1382\n",
      "Epoch 6/50\n",
      "140/140 [==============================] - 768s 5s/step - loss: 3.6557 - accuracy: 0.1199 - val_loss: 3.3500 - val_accuracy: 0.1616\n",
      "Epoch 7/50\n",
      "140/140 [==============================] - 601s 4s/step - loss: 3.3564 - accuracy: 0.1634 - val_loss: 2.9928 - val_accuracy: 0.2334\n",
      "Epoch 8/50\n",
      "140/140 [==============================] - 376s 3s/step - loss: 3.1184 - accuracy: 0.2007 - val_loss: 2.7125 - val_accuracy: 0.2926\n",
      "Epoch 9/50\n",
      "140/140 [==============================] - 384s 3s/step - loss: 2.7765 - accuracy: 0.2652 - val_loss: 2.4765 - val_accuracy: 0.3178\n",
      "Epoch 10/50\n",
      "140/140 [==============================] - 367s 3s/step - loss: 2.5100 - accuracy: 0.3256 - val_loss: 2.3132 - val_accuracy: 0.3680\n",
      "Epoch 11/50\n",
      "140/140 [==============================] - 345s 2s/step - loss: 2.2232 - accuracy: 0.3907 - val_loss: 1.9772 - val_accuracy: 0.4776\n",
      "Epoch 12/50\n",
      "140/140 [==============================] - 362s 3s/step - loss: 1.9561 - accuracy: 0.4495 - val_loss: 1.8570 - val_accuracy: 0.4937\n",
      "Epoch 13/50\n",
      "140/140 [==============================] - 550s 4s/step - loss: 1.7667 - accuracy: 0.4980 - val_loss: 1.6767 - val_accuracy: 0.5009\n",
      "Epoch 14/50\n",
      "140/140 [==============================] - 601s 4s/step - loss: 1.5559 - accuracy: 0.5595 - val_loss: 1.4912 - val_accuracy: 0.5943\n",
      "Epoch 15/50\n",
      "140/140 [==============================] - 522s 4s/step - loss: 1.3562 - accuracy: 0.6161 - val_loss: 1.4018 - val_accuracy: 0.6122\n",
      "Epoch 16/50\n",
      "140/140 [==============================] - 707s 5s/step - loss: 1.2433 - accuracy: 0.6399 - val_loss: 1.3720 - val_accuracy: 0.6032\n",
      "Epoch 17/50\n",
      "140/140 [==============================] - 523s 4s/step - loss: 1.1230 - accuracy: 0.6778 - val_loss: 1.2762 - val_accuracy: 0.6266\n",
      "Epoch 18/50\n",
      "140/140 [==============================] - 404s 3s/step - loss: 0.9951 - accuracy: 0.7173 - val_loss: 1.1612 - val_accuracy: 0.6786\n",
      "Epoch 19/50\n",
      "140/140 [==============================] - 413s 3s/step - loss: 0.9101 - accuracy: 0.7371 - val_loss: 1.1004 - val_accuracy: 0.6840\n",
      "Epoch 20/50\n",
      "140/140 [==============================] - 414s 3s/step - loss: 0.8175 - accuracy: 0.7647 - val_loss: 1.0302 - val_accuracy: 0.6930\n",
      "Epoch 21/50\n",
      "140/140 [==============================] - 412s 3s/step - loss: 0.7278 - accuracy: 0.7925 - val_loss: 1.0006 - val_accuracy: 0.7217\n",
      "Epoch 22/50\n",
      "140/140 [==============================] - 408s 3s/step - loss: 0.6633 - accuracy: 0.8123 - val_loss: 0.9859 - val_accuracy: 0.7217\n",
      "Epoch 23/50\n",
      "140/140 [==============================] - 387s 3s/step - loss: 0.5927 - accuracy: 0.8271 - val_loss: 0.9962 - val_accuracy: 0.7199\n",
      "Epoch 24/50\n",
      "140/140 [==============================] - 388s 3s/step - loss: 0.5454 - accuracy: 0.8487 - val_loss: 0.9497 - val_accuracy: 0.7469\n",
      "Epoch 25/50\n",
      "140/140 [==============================] - 409s 3s/step - loss: 0.4718 - accuracy: 0.8669 - val_loss: 0.9102 - val_accuracy: 0.7504\n",
      "Epoch 26/50\n",
      "140/140 [==============================] - 734s 5s/step - loss: 0.4427 - accuracy: 0.8738 - val_loss: 1.0009 - val_accuracy: 0.7092\n",
      "Epoch 27/50\n",
      "140/140 [==============================] - 781s 6s/step - loss: 0.4078 - accuracy: 0.8866 - val_loss: 1.0234 - val_accuracy: 0.6876\n",
      "Epoch 28/50\n",
      "140/140 [==============================] - 778s 6s/step - loss: 0.3614 - accuracy: 0.9005 - val_loss: 0.9172 - val_accuracy: 0.7487\n",
      "Epoch 29/50\n",
      "140/140 [==============================] - 810s 6s/step - loss: 0.3027 - accuracy: 0.9207 - val_loss: 0.9202 - val_accuracy: 0.7504\n",
      "Epoch 30/50\n",
      "140/140 [==============================] - 574s 4s/step - loss: 0.2974 - accuracy: 0.9219 - val_loss: 0.9286 - val_accuracy: 0.7397\n",
      "Epoch 31/50\n",
      "140/140 [==============================] - 400s 3s/step - loss: 0.3054 - accuracy: 0.9095 - val_loss: 0.9861 - val_accuracy: 0.7271\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=6,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_tr_data,      # encoder input\n",
    "    y_tr,           # target word labels\n",
    "    validation_data=(X_val_data, y_val),\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "     callbacks=[early_stop]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfd0de7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 14s 793ms/step - loss: 1.0114 - accuracy: 0.7074\n",
      "Test accuracy: 0.7073608636856079\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_te_data, y_te)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57013f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sign_bert_model.plt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sign_bert_model.plt\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"sign_bert_model.plt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eae995af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sign_bert_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf853bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 17s 902ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat=model.predict(X_te_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5875442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ytrue = np.argmax( y_test_encoded, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56e9f419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41,\n",
       " 2,\n",
       " 18,\n",
       " 105,\n",
       " 11,\n",
       " 83,\n",
       " 108,\n",
       " 14,\n",
       " 108,\n",
       " 24,\n",
       " 125,\n",
       " 61,\n",
       " 121,\n",
       " 132,\n",
       " 19,\n",
       " 13,\n",
       " 61,\n",
       " 91,\n",
       " 71,\n",
       " 135,\n",
       " 18,\n",
       " 76,\n",
       " 33,\n",
       " 142,\n",
       " 12,\n",
       " 23,\n",
       " 126,\n",
       " 29,\n",
       " 114,\n",
       " 11,\n",
       " 109,\n",
       " 47,\n",
       " 102,\n",
       " 51,\n",
       " 141,\n",
       " 123,\n",
       " 7,\n",
       " 47,\n",
       " 44,\n",
       " 93,\n",
       " 138,\n",
       " 0,\n",
       " 68,\n",
       " 87,\n",
       " 104,\n",
       " 35,\n",
       " 47,\n",
       " 64,\n",
       " 119,\n",
       " 11,\n",
       " 5,\n",
       " 37,\n",
       " 14,\n",
       " 102,\n",
       " 104,\n",
       " 72,\n",
       " 79,\n",
       " 126,\n",
       " 123,\n",
       " 128,\n",
       " 9,\n",
       " 32,\n",
       " 101,\n",
       " 29,\n",
       " 137,\n",
       " 79,\n",
       " 134,\n",
       " 17,\n",
       " 35,\n",
       " 4,\n",
       " 137,\n",
       " 107,\n",
       " 83,\n",
       " 61,\n",
       " 38,\n",
       " 84,\n",
       " 65,\n",
       " 42,\n",
       " 33,\n",
       " 138,\n",
       " 91,\n",
       " 116,\n",
       " 122,\n",
       " 9,\n",
       " 125,\n",
       " 4,\n",
       " 40,\n",
       " 89,\n",
       " 92,\n",
       " 48,\n",
       " 134,\n",
       " 21,\n",
       " 47,\n",
       " 18,\n",
       " 113,\n",
       " 12,\n",
       " 14,\n",
       " 45,\n",
       " 101,\n",
       " 56,\n",
       " 121,\n",
       " 105,\n",
       " 134,\n",
       " 26,\n",
       " 16,\n",
       " 5,\n",
       " 44,\n",
       " 76,\n",
       " 44,\n",
       " 30,\n",
       " 22,\n",
       " 18,\n",
       " 47,\n",
       " 33,\n",
       " 3,\n",
       " 38,\n",
       " 131,\n",
       " 35,\n",
       " 98,\n",
       " 130,\n",
       " 67,\n",
       " 64,\n",
       " 130,\n",
       " 96,\n",
       " 33,\n",
       " 34,\n",
       " 144,\n",
       " 112,\n",
       " 33,\n",
       " 30,\n",
       " 119,\n",
       " 95,\n",
       " 52,\n",
       " 65,\n",
       " 139,\n",
       " 82,\n",
       " 95,\n",
       " 126,\n",
       " 56,\n",
       " 121,\n",
       " 132,\n",
       " 48,\n",
       " 96,\n",
       " 100,\n",
       " 106,\n",
       " 28,\n",
       " 14,\n",
       " 33,\n",
       " 28,\n",
       " 21,\n",
       " 104,\n",
       " 96,\n",
       " 38,\n",
       " 27,\n",
       " 38,\n",
       " 38,\n",
       " 143,\n",
       " 82,\n",
       " 133,\n",
       " 127,\n",
       " 0,\n",
       " 25,\n",
       " 76,\n",
       " 141,\n",
       " 51,\n",
       " 144,\n",
       " 142,\n",
       " 130,\n",
       " 9,\n",
       " 4,\n",
       " 51,\n",
       " 33,\n",
       " 20,\n",
       " 57,\n",
       " 11,\n",
       " 76,\n",
       " 138,\n",
       " 35,\n",
       " 89,\n",
       " 27,\n",
       " 77,\n",
       " 144,\n",
       " 116,\n",
       " 8,\n",
       " 48,\n",
       " 25,\n",
       " 26,\n",
       " 72,\n",
       " 75,\n",
       " 131,\n",
       " 110,\n",
       " 91,\n",
       " 61,\n",
       " 35,\n",
       " 74,\n",
       " 91,\n",
       " 125,\n",
       " 32,\n",
       " 21,\n",
       " 79,\n",
       " 112,\n",
       " 84,\n",
       " 105,\n",
       " 38,\n",
       " 41,\n",
       " 118,\n",
       " 77,\n",
       " 125,\n",
       " 117,\n",
       " 59,\n",
       " 44,\n",
       " 114,\n",
       " 54,\n",
       " 33,\n",
       " 78,\n",
       " 126,\n",
       " 90,\n",
       " 42,\n",
       " 73,\n",
       " 113,\n",
       " 82,\n",
       " 114,\n",
       " 14,\n",
       " 74,\n",
       " 102,\n",
       " 70,\n",
       " 101,\n",
       " 50,\n",
       " 24,\n",
       " 145,\n",
       " 53,\n",
       " 42,\n",
       " 94,\n",
       " 123,\n",
       " 67,\n",
       " 50,\n",
       " 110,\n",
       " 62,\n",
       " 142,\n",
       " 75,\n",
       " 101,\n",
       " 120,\n",
       " 106,\n",
       " 18,\n",
       " 45,\n",
       " 46,\n",
       " 32,\n",
       " 3,\n",
       " 40,\n",
       " 97,\n",
       " 70,\n",
       " 0,\n",
       " 112,\n",
       " 14,\n",
       " 47,\n",
       " 127,\n",
       " 90,\n",
       " 68,\n",
       " 5,\n",
       " 90,\n",
       " 126,\n",
       " 145,\n",
       " 6,\n",
       " 75,\n",
       " 21,\n",
       " 122,\n",
       " 33,\n",
       " 20,\n",
       " 38,\n",
       " 83,\n",
       " 107,\n",
       " 0,\n",
       " 97,\n",
       " 6,\n",
       " 17,\n",
       " 21,\n",
       " 35,\n",
       " 8,\n",
       " 55,\n",
       " 86,\n",
       " 103,\n",
       " 96,\n",
       " 10,\n",
       " 48,\n",
       " 42,\n",
       " 76,\n",
       " 68,\n",
       " 18,\n",
       " 76,\n",
       " 101,\n",
       " 90,\n",
       " 68,\n",
       " 24,\n",
       " 47,\n",
       " 53,\n",
       " 60,\n",
       " 47,\n",
       " 47,\n",
       " 96,\n",
       " 54,\n",
       " 17,\n",
       " 126,\n",
       " 80,\n",
       " 41,\n",
       " 40,\n",
       " 102,\n",
       " 139,\n",
       " 4,\n",
       " 58,\n",
       " 31,\n",
       " 46,\n",
       " 13,\n",
       " 119,\n",
       " 94,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 49,\n",
       " 65,\n",
       " 21,\n",
       " 129,\n",
       " 123,\n",
       " 121,\n",
       " 5,\n",
       " 8,\n",
       " 62,\n",
       " 82,\n",
       " 88,\n",
       " 120,\n",
       " 60,\n",
       " 112,\n",
       " 137,\n",
       " 38,\n",
       " 44,\n",
       " 117,\n",
       " 44,\n",
       " 12,\n",
       " 7,\n",
       " 85,\n",
       " 78,\n",
       " 90,\n",
       " 114,\n",
       " 29,\n",
       " 41,\n",
       " 88,\n",
       " 29,\n",
       " 125,\n",
       " 110,\n",
       " 74,\n",
       " 59,\n",
       " 96,\n",
       " 86,\n",
       " 58,\n",
       " 122,\n",
       " 55,\n",
       " 46,\n",
       " 131,\n",
       " 74,\n",
       " 127,\n",
       " 79,\n",
       " 33,\n",
       " 126,\n",
       " 31,\n",
       " 87,\n",
       " 73,\n",
       " 96,\n",
       " 26,\n",
       " 10,\n",
       " 60,\n",
       " 42,\n",
       " 74,\n",
       " 85,\n",
       " 10,\n",
       " 33,\n",
       " 99,\n",
       " 109,\n",
       " 130,\n",
       " 86,\n",
       " 107,\n",
       " 38,\n",
       " 86,\n",
       " 112,\n",
       " 47,\n",
       " 140,\n",
       " 134,\n",
       " 33,\n",
       " 92,\n",
       " 28,\n",
       " 68,\n",
       " 136,\n",
       " 93,\n",
       " 59,\n",
       " 93,\n",
       " 145,\n",
       " 98,\n",
       " 33,\n",
       " 130,\n",
       " 138,\n",
       " 39,\n",
       " 125,\n",
       " 142,\n",
       " 37,\n",
       " 127,\n",
       " 91,\n",
       " 96,\n",
       " 129,\n",
       " 118,\n",
       " 44,\n",
       " 98,\n",
       " 5,\n",
       " 26,\n",
       " 1,\n",
       " 127,\n",
       " 141,\n",
       " 4,\n",
       " 15,\n",
       " 6,\n",
       " 0,\n",
       " 67,\n",
       " 124,\n",
       " 6,\n",
       " 60,\n",
       " 53,\n",
       " 26,\n",
       " 115,\n",
       " 29,\n",
       " 86,\n",
       " 144,\n",
       " 60,\n",
       " 97,\n",
       " 142,\n",
       " 17,\n",
       " 144,\n",
       " 38,\n",
       " 19,\n",
       " 77,\n",
       " 6,\n",
       " 15,\n",
       " 47,\n",
       " 122,\n",
       " 56,\n",
       " 87,\n",
       " 82,\n",
       " 2,\n",
       " 120,\n",
       " 140,\n",
       " 25,\n",
       " 50,\n",
       " 76,\n",
       " 47,\n",
       " 98,\n",
       " 112,\n",
       " 17,\n",
       " 122,\n",
       " 46,\n",
       " 36,\n",
       " 118,\n",
       " 90,\n",
       " 126,\n",
       " 79,\n",
       " 103,\n",
       " 77,\n",
       " 121,\n",
       " 114,\n",
       " 47,\n",
       " 113,\n",
       " 144,\n",
       " 70,\n",
       " 145,\n",
       " 38,\n",
       " 84,\n",
       " 39,\n",
       " 98,\n",
       " 36,\n",
       " 112,\n",
       " 0,\n",
       " 43,\n",
       " 125,\n",
       " 18,\n",
       " 101,\n",
       " 89,\n",
       " 9,\n",
       " 84,\n",
       " 61,\n",
       " 68,\n",
       " 4,\n",
       " 59,\n",
       " 127,\n",
       " 115,\n",
       " 69,\n",
       " 45,\n",
       " 59,\n",
       " 61,\n",
       " 25,\n",
       " 68,\n",
       " 117,\n",
       " 142,\n",
       " 78,\n",
       " 135,\n",
       " 98,\n",
       " 96,\n",
       " 25,\n",
       " 109,\n",
       " 69,\n",
       " 36,\n",
       " 68,\n",
       " 62,\n",
       " 63,\n",
       " 108,\n",
       " 9,\n",
       " 42,\n",
       " 56,\n",
       " 81,\n",
       " 117,\n",
       " 29,\n",
       " 125,\n",
       " 127,\n",
       " 143,\n",
       " 40,\n",
       " 102,\n",
       " 91,\n",
       " 132,\n",
       " 128,\n",
       " 10,\n",
       " 105,\n",
       " 45,\n",
       " 42,\n",
       " 34,\n",
       " 101,\n",
       " 46,\n",
       " 38,\n",
       " 39,\n",
       " 135,\n",
       " 69,\n",
       " 127,\n",
       " 117,\n",
       " 104,\n",
       " 60,\n",
       " 79,\n",
       " 107,\n",
       " 128,\n",
       " 35,\n",
       " 144,\n",
       " 87,\n",
       " 102,\n",
       " 63,\n",
       " 22,\n",
       " 52,\n",
       " 4,\n",
       " 69,\n",
       " 13,\n",
       " 82,\n",
       " 127,\n",
       " 68,\n",
       " 23,\n",
       " 122,\n",
       " 139]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78b7888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample = np.expand_dims(X_te_data[4], axis=0)\n",
    "y_predicted_first = model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38020d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 146)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_first.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b04cfa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_predicted_first[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b072557c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fork'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label = le.inverse_transform([np.argmax(y_predicted_first[0])])[0]\n",
    "predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de4f39d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[550,   1],\n",
       "        [  1,   5]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[548,   6],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[550,   1],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[551,   3],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[551,   3],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[550,   1],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[548,   2],\n",
       "        [  2,   5]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[546,   2],\n",
       "        [  5,   4]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[551,   3],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[551,   3],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[543,   2],\n",
       "        [  1,  11]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[550,   1],\n",
       "        [  0,   6]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[543,   2],\n",
       "        [  2,  10]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[551,   0],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[550,   0],\n",
       "        [  0,   7]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[550,   4],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[552,   1],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[543,   2],\n",
       "        [  1,  11]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  3,   0]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  3,   0]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[550,   1],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[551,   3],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[551,   3],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  3,   0]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[548,   2],\n",
       "        [  0,   7]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[550,   4],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[548,   3],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[551,   3],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[551,   3],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[546,   4],\n",
       "        [  5,   2]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[550,   1],\n",
       "        [  1,   5]],\n",
       "\n",
       "       [[551,   3],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[550,   1],\n",
       "        [  5,   1]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[553,   0],\n",
       "        [  2,   2]],\n",
       "\n",
       "       [[546,   5],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[550,   1],\n",
       "        [  1,   5]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[550,   4],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[550,   0],\n",
       "        [  1,   6]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[551,   3],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  3,   0]],\n",
       "\n",
       "       [[548,   3],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[551,   0],\n",
       "        [  1,   5]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[550,   1],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[549,   2],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[548,   2],\n",
       "        [  1,   6]],\n",
       "\n",
       "       [[547,   4],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[548,   2],\n",
       "        [  0,   7]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[549,   1],\n",
       "        [  3,   4]],\n",
       "\n",
       "       [[548,   3],\n",
       "        [  6,   0]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  3,   0]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[552,   1],\n",
       "        [  2,   2]],\n",
       "\n",
       "       [[554,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[549,   5],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[553,   1],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[549,   5],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[552,   2],\n",
       "        [  1,   2]]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "multilabel_confusion_matrix(y_te, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6462a0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7073608617594255"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_te, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2beab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict(land_marks):\n",
    "    sample = np.expand_dims(land_marks, axis=0)\n",
    "    y_predicted = model.predict(sample)\n",
    "    predicted_index = np.argmax(y_predicted[0])\n",
    "    predicted_label = le.inverse_transform([predicted_index])[0]\n",
    "    confidence = float(np.max(y_predicted[0]))\n",
    "    out_put={\"word\":predicted_label, \"confidence\":confidence}\n",
    "    return out_put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10151962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'understand', 'confidence': 0.7880738377571106}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X_te_data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1188d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grandfather'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([y_te[100]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50f732",
   "metadata": {},
   "source": [
    "### Testing in real time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73d261ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import itertools  # <-- missing import\n",
    "\n",
    "# =========================\n",
    "# CONSTANTS\n",
    "# =========================\n",
    "POSE_LANDMARKS = 33\n",
    "HAND_LANDMARKS = 21\n",
    "FACE_LANDMARKS = 60\n",
    "\n",
    "# =========================\n",
    "# INIT MEDIAPIPE\n",
    "# =========================\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh  # <-- missing\n",
    "\n",
    "holistic = mp_holistic.Holistic(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    smooth_landmarks=True,\n",
    "    enable_segmentation=False,\n",
    "    refine_face_landmarks=True\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# FACE MESH INDICES\n",
    "# =========================\n",
    "FACEMESH_LIPS = set(itertools.chain(*mp_face_mesh.FACEMESH_LIPS))\n",
    "FACEMESH_LEFT_EYEBROW = set(itertools.chain(*mp_face_mesh.FACEMESH_LEFT_EYEBROW))\n",
    "FACEMESH_RIGHT_EYEBROW = set(itertools.chain(*mp_face_mesh.FACEMESH_RIGHT_EYEBROW))\n",
    "\n",
    "RELEVANT_FACE_INDICES = list(FACEMESH_LIPS | FACEMESH_LEFT_EYEBROW | FACEMESH_RIGHT_EYEBROW)\n",
    "RELEVANT_FACE_INDICES.sort()\n",
    "\n",
    "# Replace with your 60 indices if needed\n",
    "FACE_INDICES = list(range(60))\n",
    "\n",
    "# =========================\n",
    "# LANDMARK EXTRACTION\n",
    "# =========================\n",
    "def extract_landmarks(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image_rgb)\n",
    "\n",
    "    # ---- POSE ----\n",
    "    if results.pose_landmarks:\n",
    "        pose = np.array([\n",
    "            [lm.x, lm.y, lm.z, lm.visibility]\n",
    "            for lm in results.pose_landmarks.landmark\n",
    "        ]).flatten()\n",
    "    else:\n",
    "        pose = np.zeros(POSE_LANDMARKS * 4)\n",
    "\n",
    "    # ---- LEFT HAND ----\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand = np.array([\n",
    "            [lm.x, lm.y, lm.z]\n",
    "            for lm in results.left_hand_landmarks.landmark\n",
    "        ]).flatten()\n",
    "    else:\n",
    "        left_hand = np.zeros(HAND_LANDMARKS * 3)\n",
    "\n",
    "    # ---- RIGHT HAND ----\n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand = np.array([\n",
    "            [lm.x, lm.y, lm.z]\n",
    "            for lm in results.right_hand_landmarks.landmark\n",
    "        ]).flatten()\n",
    "    else:\n",
    "        right_hand = np.zeros(HAND_LANDMARKS * 3)\n",
    "\n",
    "    # ---- FACE (LIPS + EYEBROWS ONLY) ----\n",
    "    if results.face_landmarks:\n",
    "        relevant = [results.face_landmarks.landmark[i] for i in RELEVANT_FACE_INDICES]\n",
    "        face = np.array([[lm.x, lm.y, lm.z] for lm in relevant]).flatten()\n",
    "    else:\n",
    "        face = np.zeros(len(RELEVANT_FACE_INDICES) * 3)\n",
    "\n",
    "    # IMPORTANT: order must match training\n",
    "    landmarks = np.concatenate([pose, face, left_hand, right_hand])\n",
    "\n",
    "    return landmarks, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2487e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "# =========================\n",
    "# REAL-TIME WEBCAM LOOP\n",
    "# =========================\n",
    "SEQUENCE_LENGTH = 157  # same as training\n",
    "sequence = []\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    landmarks, results = extract_landmarks(frame)\n",
    "    sequence.append(landmarks)\n",
    "\n",
    "    # Keep only last SEQUENCE_LENGTH frames\n",
    "    if len(sequence) > SEQUENCE_LENGTH:\n",
    "        sequence = sequence[-SEQUENCE_LENGTH:]\n",
    "\n",
    "    # ================= DRAW LANDMARKS =================\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "    # Draw lips + eyebrows only\n",
    "    if results.face_landmarks:\n",
    "        face_connections = list(mp_face_mesh.FACEMESH_LIPS) + \\\n",
    "                           list(mp_face_mesh.FACEMESH_LEFT_EYEBROW) + \\\n",
    "                           list(mp_face_mesh.FACEMESH_RIGHT_EYEBROW)\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            results.face_landmarks,\n",
    "            connections=face_connections,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=1)\n",
    "        )\n",
    "\n",
    "    # ================= PREDICT =================\n",
    "    if len(sequence) == SEQUENCE_LENGTH:\n",
    "        predicted = predict(sequence)  # Your SignBERT predict function here\n",
    "\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"Pred: {predicted['word']}  Conf: {predicted['confidence']}\",\n",
    "             \n",
    "            (10, 40),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (0, 255, 0),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    cv2.imshow(\"SignBERT Real-Time\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signlanguage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
