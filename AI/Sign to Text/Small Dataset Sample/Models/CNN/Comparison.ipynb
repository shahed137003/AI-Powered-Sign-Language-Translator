{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c57eb8-3a72-474e-9992-9a40b3afe941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Evaluating model: Hands Only + Sliding\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CNN1D:\n\tsize mismatch for conv_block.0.weight: copying a param with shape torch.Size([256, 438, 3]) from checkpoint, the shape in current model is torch.Size([256, 258, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 187\u001b[0m\n\u001b[0;32m    184\u001b[0m FEATURE_DIM_MODEL \u001b[38;5;241m=\u001b[39m FEATURE_DIMS[name]\n\u001b[0;32m    185\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN1D(FEATURE_DIM_MODEL, num_classes)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m--> 187\u001b[0m acc, all_preds_str, all_labels_str, probs \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m results[name] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: acc,\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_preds\u001b[39m\u001b[38;5;124m\"\u001b[39m: all_preds_str,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_classes\n\u001b[0;32m    195\u001b[0m }\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 141\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, test_loader, label_encoder, model_path)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_model\u001b[39m(model, test_loader, label_encoder, model_path):\n\u001b[1;32m--> 141\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_with_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    144\u001b[0m     all_preds, all_labels \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[10], line 134\u001b[0m, in \u001b[0;36mload_model_with_compatibility\u001b[1;34m(model, path)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m         new_state_dict[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m--> 134\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\Desktop\\New folder (2)\\venv310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2635\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2627\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2630\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2631\u001b[0m             ),\n\u001b[0;32m   2632\u001b[0m         )\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2637\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2638\u001b[0m         )\n\u001b[0;32m   2639\u001b[0m     )\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CNN1D:\n\tsize mismatch for conv_block.0.weight: copying a param with shape torch.Size([256, 438, 3]) from checkpoint, the shape in current model is torch.Size([256, 258, 3])."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "DATA_DIRS = {\n",
    "    \"Hands Only + Sliding\": r\"E:\\ASL_Citizen\\NEW\\Top_Classes_Landmarks_Preprocessed\",\n",
    "    \"All Features + Sliding\": r\"E:\\ASL_Citizen\\NEW\\Top_Classes_Landmarks_Preprocessed\",\n",
    "    \"All Features w/o Sliding\": r\"E:\\ASL_Citizen\\NEW\\Top_Classes_Landmarks_Preprocessed_method2\",\n",
    "    \"Hands + Face + Sliding\": r\"E:\\ASL_Citizen\\NEW\\Top_Classes_Landmarks_Preprocessed\",\n",
    "    \"Hands + Pose + Sliding\": r\"E:\\ASL_Citizen\\NEW\\Top_Classes_Landmarks_Preprocessed\"\n",
    "}\n",
    "\n",
    "MODEL_PATHS = {\n",
    "    \"Hands Only + Sliding\": \"CNN_hands_only_sliding_mask_best_model.pth\",\n",
    "    \"All Features + Sliding\": \"CNN_with_sliding&mask_best_model.pth\",\n",
    "    \"All Features w/o Sliding\": \"CNN_without_sliding&mask_best_model.pth\",\n",
    "    \"Hands + Face + Sliding\": \"CNN_hands_face_sliding_mask_best_model.pth\",\n",
    "    \"Hands + Pose + Sliding\": \"CNN_hands_pose_sliding_mask_best_model.pth\"\n",
    "}\n",
    "\n",
    "FEATURE_DIMS = {\n",
    "    \"Hands Only + Sliding\": 258,  # Example: trained with 258 features\n",
    "    \"All Features + Sliding\": 438,\n",
    "    \"All Features w/o Sliding\": 438,\n",
    "    \"Hands + Face + Sliding\": 438,\n",
    "    \"Hands + Pose + Sliding\": 438\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ==============================\n",
    "# DATASET\n",
    "# ==============================\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, files, labels):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.load(self.files[idx]).astype(np.float32)\n",
    "        x = torch.tensor(x).permute(1, 0)  # (features, frames)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# ==============================\n",
    "# LOAD FILES & LABELS\n",
    "# ==============================\n",
    "def load_files_and_labels(data_dir):\n",
    "    files, labels = [], []\n",
    "    for f in os.listdir(data_dir):\n",
    "        if f.endswith(\".npy\") and \"_mask\" not in f:\n",
    "            files.append(os.path.join(data_dir, f))\n",
    "            gloss = f.rsplit(\"_\", 1)[0].split(\"_\")[0]\n",
    "            labels.append(gloss)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(labels)\n",
    "\n",
    "    # Filter classes with <2 samples\n",
    "    label_counts = Counter(y_encoded)\n",
    "    valid_idx = [i for i, y in enumerate(y_encoded) if label_counts[y] >= 2]\n",
    "\n",
    "    files = [files[i] for i in valid_idx]\n",
    "    y_encoded = y_encoded[valid_idx]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_encoded)\n",
    "    num_classes = len(le.classes_)\n",
    "\n",
    "    return files, y_encoded, num_classes, le\n",
    "\n",
    "# ==============================\n",
    "# CNN MODEL\n",
    "# ==============================\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(input_features, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Conv1d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.05),\n",
    "\n",
    "            nn.Conv1d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ==============================\n",
    "# LOAD MODEL WITH CHECKPOINT COMPATIBILITY\n",
    "# ==============================\n",
    "def load_model_with_compatibility(model, path):\n",
    "    \"\"\"\n",
    "    Loads a checkpoint while fixing key name differences\n",
    "    (conv vs conv_block) from old models.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path, map_location=DEVICE)\n",
    "    new_state_dict = {}\n",
    "\n",
    "    for k, v in checkpoint.items():\n",
    "        if k.startswith(\"conv.\"):\n",
    "            new_key = k.replace(\"conv.\", \"conv_block.\")\n",
    "            new_state_dict[new_key] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    return model\n",
    "\n",
    "# ==============================\n",
    "# EVALUATION FUNCTION\n",
    "# ==============================\n",
    "def evaluate_model(model, test_loader, label_encoder, model_path):\n",
    "    model = load_model_with_compatibility(model, model_path)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # Convert to string labels\n",
    "    all_preds_str = label_encoder.inverse_transform(all_preds)\n",
    "    all_labels_str = label_encoder.inverse_transform(all_labels)\n",
    "\n",
    "    return acc, all_preds_str, all_labels_str, np.array([torch.softmax(model(x.to(DEVICE)), dim=1).cpu().numpy() for x, _ in test_loader]).reshape(-1, len(label_encoder.classes_))\n",
    "\n",
    "# ==============================\n",
    "# MAIN LOOP\n",
    "# ==============================\n",
    "results = {}\n",
    "\n",
    "for name, model_path in MODEL_PATHS.items():\n",
    "    print(f\"\\nEvaluating model: {name}\")\n",
    "    data_dir = DATA_DIRS[name]\n",
    "    files, y_encoded, num_classes, le = load_files_and_labels(data_dir)\n",
    "\n",
    "    # Train/Val/Test split\n",
    "    files_train, files_tmp, y_train, y_tmp = train_test_split(\n",
    "        files, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    "    )\n",
    "    files_val, files_test, y_val, y_test = train_test_split(\n",
    "        files_tmp, y_tmp, test_size=0.5, stratify=y_tmp, random_state=42\n",
    "    )\n",
    "\n",
    "    test_dataset = LandmarkDataset(files_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Use correct feature dimension\n",
    "    FEATURE_DIM_MODEL = FEATURE_DIMS[name]\n",
    "    model = CNN1D(FEATURE_DIM_MODEL, num_classes).to(DEVICE)\n",
    "\n",
    "    acc, all_preds_str, all_labels_str, probs = evaluate_model(model, test_loader, le, model_path)\n",
    "\n",
    "    results[name] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"all_preds\": all_preds_str,\n",
    "        \"all_labels\": all_labels_str,\n",
    "        \"probs\": probs,\n",
    "        \"num_classes\": num_classes\n",
    "    }\n",
    "\n",
    "    print(f\"{name} Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# VISUALIZATIONS\n",
    "# ==============================\n",
    "\n",
    "# 1️⃣ Overall Accuracy\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(results.keys(), [r[\"accuracy\"] for r in results.values()], color='skyblue')\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Overall Test Accuracy Comparison\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0,1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2️⃣ Per-Class Accuracy Heatmap (Matplotlib)\n",
    "plt.figure(figsize=(12,6))\n",
    "max_classes = max(r[\"num_classes\"] for r in results.values())\n",
    "per_class_matrix = np.zeros((len(results), max_classes))\n",
    "\n",
    "for i, r in enumerate(results.values()):\n",
    "    # Compute per-class accuracy\n",
    "    per_class_acc = []\n",
    "    for c in np.unique(r[\"all_labels\"]):\n",
    "        idxs = [i for i, lbl in enumerate(r[\"all_labels\"]) if lbl == c]\n",
    "        acc_c = np.mean([r[\"all_preds\"][i] == r[\"all_labels\"][i] for i in idxs])\n",
    "        per_class_acc.append(acc_c)\n",
    "    per_class_matrix[i, :len(per_class_acc)] = per_class_acc\n",
    "\n",
    "plt.imshow(per_class_matrix, aspect='auto', cmap='YlGnBu', vmin=0, vmax=1)\n",
    "plt.colorbar(label=\"Accuracy\")\n",
    "plt.yticks(range(len(results)), results.keys())\n",
    "plt.xticks(range(max_classes), [f\"C{i}\" for i in range(max_classes)], rotation=90)\n",
    "plt.xlabel(\"Class Index\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.title(\"Per-Class Accuracy Comparison\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3️⃣ Prediction Confidence Distribution\n",
    "plt.figure(figsize=(12,6))\n",
    "x_vals = np.linspace(0,1,200)\n",
    "for name, r in results.items():\n",
    "    top1_probs = r[\"probs\"].max(axis=1)\n",
    "    kde = gaussian_kde(top1_probs)\n",
    "    plt.plot(x_vals, kde(x_vals), label=name)\n",
    "plt.xlabel(\"Prediction Confidence (Top-1)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Prediction Confidence Distribution\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf94e27-c213-4df0-93ff-6b923a6a8e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
