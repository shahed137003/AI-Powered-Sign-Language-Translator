{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d5867e-2ee5-4089-9cad-5e5fdff57539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5568\n",
      "Classes: 146\n",
      "Input dim: 270\n",
      "\n",
      "Epoch 1/350\n",
      "Train Loss 5.0248 | Acc 0.0143\n",
      "Val   Loss 4.9225 | Acc 0.0099\n",
      "Saved best model\n",
      "\n",
      "Epoch 2/350\n",
      "Train Loss 4.7544 | Acc 0.0287\n",
      "Val   Loss 4.6995 | Acc 0.0211\n",
      "Saved best model\n",
      "\n",
      "Epoch 3/350\n",
      "Train Loss 4.5228 | Acc 0.0507\n",
      "Val   Loss 4.2603 | Acc 0.0873\n",
      "Saved best model\n",
      "\n",
      "Epoch 4/350\n",
      "Train Loss 4.3645 | Acc 0.0652\n",
      "Val   Loss 4.2969 | Acc 0.0817\n",
      "\n",
      "Epoch 5/350\n",
      "Train Loss 4.1959 | Acc 0.0930\n",
      "Val   Loss 3.8932 | Acc 0.1338\n",
      "Saved best model\n",
      "\n",
      "Epoch 6/350\n",
      "Train Loss 4.0752 | Acc 0.1226\n",
      "Val   Loss 3.8642 | Acc 0.1437\n",
      "Saved best model\n",
      "\n",
      "Epoch 7/350\n",
      "Train Loss 4.0160 | Acc 0.1458\n",
      "Val   Loss 3.6936 | Acc 0.2028\n",
      "Saved best model\n",
      "\n",
      "Epoch 8/350\n",
      "Train Loss 3.9175 | Acc 0.1635\n",
      "Val   Loss 3.5728 | Acc 0.2310\n",
      "Saved best model\n",
      "\n",
      "Epoch 9/350\n",
      "Train Loss 3.8642 | Acc 0.1841\n",
      "Val   Loss 3.6272 | Acc 0.2070\n",
      "\n",
      "Epoch 10/350\n",
      "Train Loss 3.7773 | Acc 0.2052\n",
      "Val   Loss 3.4036 | Acc 0.2648\n",
      "Saved best model\n",
      "\n",
      "Epoch 11/350\n",
      "Train Loss 3.7645 | Acc 0.1998\n",
      "Val   Loss 3.4302 | Acc 0.2535\n",
      "\n",
      "Epoch 12/350\n",
      "Train Loss 3.7011 | Acc 0.2264\n",
      "Val   Loss 3.2166 | Acc 0.3282\n",
      "Saved best model\n",
      "\n",
      "Epoch 13/350\n",
      "Train Loss 3.5828 | Acc 0.2593\n",
      "Val   Loss 3.1632 | Acc 0.3465\n",
      "Saved best model\n",
      "\n",
      "Epoch 14/350\n",
      "Train Loss 3.5165 | Acc 0.2737\n",
      "Val   Loss 3.0756 | Acc 0.3958\n",
      "Saved best model\n",
      "\n",
      "Epoch 15/350\n",
      "Train Loss 3.6222 | Acc 0.2547\n",
      "Val   Loss 3.0446 | Acc 0.3887\n",
      "\n",
      "Epoch 16/350\n",
      "Train Loss 3.5752 | Acc 0.2622\n",
      "Val   Loss 3.1171 | Acc 0.3648\n",
      "\n",
      "Epoch 17/350\n",
      "Train Loss 3.5240 | Acc 0.2798\n",
      "Val   Loss 3.0676 | Acc 0.3831\n",
      "\n",
      "Epoch 18/350\n",
      "Train Loss 3.4044 | Acc 0.2958\n",
      "Val   Loss 2.9833 | Acc 0.4127\n",
      "Saved best model\n",
      "\n",
      "Epoch 19/350\n",
      "Train Loss 3.3292 | Acc 0.3187\n",
      "Val   Loss 2.9377 | Acc 0.4310\n",
      "Saved best model\n",
      "\n",
      "Epoch 20/350\n",
      "Train Loss 3.4613 | Acc 0.2960\n",
      "Val   Loss 2.9180 | Acc 0.4620\n",
      "Saved best model\n",
      "\n",
      "Epoch 21/350\n",
      "Train Loss 3.4156 | Acc 0.3070\n",
      "Val   Loss 2.9150 | Acc 0.4437\n",
      "\n",
      "Epoch 22/350\n",
      "Train Loss 3.3240 | Acc 0.3185\n",
      "Val   Loss 2.9085 | Acc 0.4408\n",
      "\n",
      "Epoch 23/350\n",
      "Train Loss 3.3197 | Acc 0.3256\n",
      "Val   Loss 2.9077 | Acc 0.4648\n",
      "Saved best model\n",
      "\n",
      "Epoch 24/350\n",
      "Train Loss 3.3640 | Acc 0.3102\n",
      "Val   Loss 2.9075 | Acc 0.4620\n",
      "\n",
      "Epoch 25/350\n",
      "Train Loss 3.5596 | Acc 0.2681\n",
      "Val   Loss 3.1691 | Acc 0.3268\n",
      "\n",
      "Epoch 26/350\n",
      "Train Loss 3.5697 | Acc 0.2684\n",
      "Val   Loss 3.1344 | Acc 0.3296\n",
      "\n",
      "Epoch 27/350\n",
      "Train Loss 3.4873 | Acc 0.2773\n",
      "Val   Loss 3.2099 | Acc 0.3028\n",
      "\n",
      "Epoch 28/350\n",
      "Train Loss 3.4643 | Acc 0.2944\n",
      "Val   Loss 3.1221 | Acc 0.3408\n",
      "\n",
      "Epoch 29/350\n",
      "Train Loss 3.3757 | Acc 0.3236\n",
      "Val   Loss 3.5833 | Acc 0.1958\n",
      "\n",
      "Epoch 30/350\n",
      "Train Loss 3.2546 | Acc 0.3470\n",
      "Val   Loss 3.0321 | Acc 0.3831\n",
      "\n",
      "Epoch 31/350\n",
      "Train Loss 3.1813 | Acc 0.3737\n",
      "Val   Loss 3.0024 | Acc 0.3704\n",
      "\n",
      "Epoch 32/350\n",
      "Train Loss 3.2461 | Acc 0.3696\n",
      "Val   Loss 3.0605 | Acc 0.3366\n",
      "\n",
      "Epoch 33/350\n",
      "Train Loss 3.2004 | Acc 0.3731\n",
      "Val   Loss 2.8213 | Acc 0.3944\n",
      "\n",
      "Epoch 34/350\n",
      "Train Loss 3.1103 | Acc 0.4025\n",
      "Val   Loss 2.7380 | Acc 0.4408\n",
      "\n",
      "Epoch 35/350\n",
      "Train Loss 3.0036 | Acc 0.4320\n",
      "Val   Loss 3.2210 | Acc 0.3268\n",
      "\n",
      "Epoch 36/350\n",
      "Train Loss 3.0640 | Acc 0.4265\n",
      "Val   Loss 2.4830 | Acc 0.5437\n",
      "Saved best model\n",
      "\n",
      "Epoch 37/350\n",
      "Train Loss 3.0003 | Acc 0.4404\n",
      "Val   Loss 2.7169 | Acc 0.4338\n",
      "\n",
      "Epoch 38/350\n",
      "Train Loss 2.9927 | Acc 0.4459\n",
      "Val   Loss 2.8260 | Acc 0.4155\n",
      "\n",
      "Epoch 39/350\n",
      "Train Loss 2.9021 | Acc 0.4627\n",
      "Val   Loss 2.7237 | Acc 0.4197\n",
      "\n",
      "Epoch 40/350\n",
      "Train Loss 2.9571 | Acc 0.4592\n",
      "Val   Loss 2.6081 | Acc 0.4817\n",
      "\n",
      "Epoch 41/350\n",
      "Train Loss 2.9816 | Acc 0.4609\n",
      "Val   Loss 2.6691 | Acc 0.4676\n",
      "\n",
      "Epoch 42/350\n",
      "Train Loss 2.8716 | Acc 0.4866\n",
      "Val   Loss 2.5703 | Acc 0.4915\n",
      "\n",
      "Epoch 43/350\n",
      "Train Loss 2.8447 | Acc 0.4967\n",
      "Val   Loss 2.3960 | Acc 0.5282\n",
      "\n",
      "Epoch 44/350\n",
      "Train Loss 2.8917 | Acc 0.4887\n",
      "Val   Loss 2.2734 | Acc 0.5831\n",
      "Saved best model\n",
      "\n",
      "Epoch 45/350\n",
      "Train Loss 2.7733 | Acc 0.5269\n",
      "Val   Loss 2.3934 | Acc 0.5521\n",
      "\n",
      "Epoch 46/350\n",
      "Train Loss 2.7853 | Acc 0.5163\n",
      "Val   Loss 2.2642 | Acc 0.6000\n",
      "Saved best model\n",
      "\n",
      "Epoch 47/350\n",
      "Train Loss 2.7328 | Acc 0.5306\n",
      "Val   Loss 2.3601 | Acc 0.5620\n",
      "\n",
      "Epoch 48/350\n",
      "Train Loss 2.7480 | Acc 0.5408\n",
      "Val   Loss 2.3934 | Acc 0.5521\n",
      "\n",
      "Epoch 49/350\n",
      "Train Loss 2.7662 | Acc 0.5342\n",
      "Val   Loss 2.1173 | Acc 0.6268\n",
      "Saved best model\n",
      "\n",
      "Epoch 50/350\n",
      "Train Loss 2.7478 | Acc 0.5351\n",
      "Val   Loss 2.2371 | Acc 0.5831\n",
      "\n",
      "Epoch 51/350\n",
      "Train Loss 2.6410 | Acc 0.5658\n",
      "Val   Loss 2.1980 | Acc 0.6070\n",
      "\n",
      "Epoch 52/350\n",
      "Train Loss 2.6560 | Acc 0.5710\n",
      "Val   Loss 2.0823 | Acc 0.6394\n",
      "Saved best model\n",
      "\n",
      "Epoch 53/350\n",
      "Train Loss 2.5519 | Acc 0.5989\n",
      "Val   Loss 2.1171 | Acc 0.6324\n",
      "\n",
      "Epoch 54/350\n",
      "Train Loss 2.5761 | Acc 0.5910\n",
      "Val   Loss 2.2094 | Acc 0.5958\n",
      "\n",
      "Epoch 55/350\n",
      "Train Loss 2.5334 | Acc 0.6030\n",
      "Val   Loss 2.0594 | Acc 0.6437\n",
      "Saved best model\n",
      "\n",
      "Epoch 56/350\n",
      "Train Loss 2.6140 | Acc 0.5776\n",
      "Val   Loss 2.0078 | Acc 0.6775\n",
      "Saved best model\n",
      "\n",
      "Epoch 57/350\n",
      "Train Loss 2.4675 | Acc 0.6223\n",
      "Val   Loss 2.0312 | Acc 0.6563\n",
      "\n",
      "Epoch 58/350\n",
      "Train Loss 2.4660 | Acc 0.6296\n",
      "Val   Loss 1.9694 | Acc 0.7070\n",
      "Saved best model\n",
      "\n",
      "Epoch 59/350\n",
      "Train Loss 2.4651 | Acc 0.6302\n",
      "Val   Loss 1.9579 | Acc 0.6761\n",
      "\n",
      "Epoch 60/350\n",
      "Train Loss 2.5142 | Acc 0.6162\n",
      "Val   Loss 1.9799 | Acc 0.6901\n",
      "\n",
      "Epoch 61/350\n",
      "Train Loss 2.5088 | Acc 0.6140\n",
      "Val   Loss 1.9240 | Acc 0.6944\n",
      "\n",
      "Epoch 62/350\n",
      "Train Loss 2.5438 | Acc 0.6123\n",
      "Val   Loss 1.9860 | Acc 0.6845\n",
      "\n",
      "Epoch 63/350\n",
      "Train Loss 2.5806 | Acc 0.6047\n",
      "Val   Loss 1.9474 | Acc 0.6859\n",
      "\n",
      "Epoch 64/350\n",
      "Train Loss 2.4186 | Acc 0.6477\n",
      "Val   Loss 1.9249 | Acc 0.6972\n",
      "\n",
      "Epoch 65/350\n",
      "Train Loss 2.5068 | Acc 0.6241\n",
      "Val   Loss 1.9401 | Acc 0.6972\n",
      "\n",
      "Epoch 66/350\n",
      "Train Loss 2.4291 | Acc 0.6518\n",
      "Val   Loss 1.9097 | Acc 0.7113\n",
      "Saved best model\n",
      "\n",
      "Epoch 67/350\n",
      "Train Loss 2.5274 | Acc 0.6160\n",
      "Val   Loss 1.9234 | Acc 0.7028\n",
      "\n",
      "Epoch 68/350\n",
      "Train Loss 2.3573 | Acc 0.6725\n",
      "Val   Loss 1.9192 | Acc 0.7028\n",
      "\n",
      "Epoch 69/350\n",
      "Train Loss 2.3715 | Acc 0.6661\n",
      "Val   Loss 1.9079 | Acc 0.7000\n",
      "\n",
      "Epoch 70/350\n",
      "Train Loss 2.3946 | Acc 0.6585\n",
      "Val   Loss 1.8777 | Acc 0.7183\n",
      "Saved best model\n",
      "\n",
      "Epoch 71/350\n",
      "Train Loss 2.4495 | Acc 0.6393\n",
      "Val   Loss 1.8800 | Acc 0.7169\n",
      "\n",
      "Epoch 72/350\n",
      "Train Loss 2.3925 | Acc 0.6553\n",
      "Val   Loss 1.8737 | Acc 0.7169\n",
      "\n",
      "Epoch 73/350\n",
      "Train Loss 2.4428 | Acc 0.6451\n",
      "Val   Loss 1.8773 | Acc 0.7127\n",
      "\n",
      "Epoch 74/350\n",
      "Train Loss 2.3508 | Acc 0.6618\n",
      "Val   Loss 1.8925 | Acc 0.7056\n",
      "\n",
      "Epoch 75/350\n",
      "Train Loss 2.3506 | Acc 0.6634\n",
      "Val   Loss 1.8798 | Acc 0.7225\n",
      "Saved best model\n",
      "\n",
      "Epoch 76/350\n",
      "Train Loss 2.4799 | Acc 0.6121\n",
      "Val   Loss 2.4984 | Acc 0.5211\n",
      "\n",
      "Epoch 77/350\n",
      "Train Loss 2.6133 | Acc 0.5801\n",
      "Val   Loss 2.3080 | Acc 0.5634\n",
      "\n",
      "Epoch 78/350\n",
      "Train Loss 2.4915 | Acc 0.6102\n",
      "Val   Loss 2.4049 | Acc 0.5352\n",
      "\n",
      "Epoch 79/350\n",
      "Train Loss 2.5148 | Acc 0.5998\n",
      "Val   Loss 2.6423 | Acc 0.4690\n",
      "\n",
      "Epoch 80/350\n",
      "Train Loss 2.5829 | Acc 0.5867\n",
      "Val   Loss 2.8450 | Acc 0.4338\n",
      "\n",
      "Epoch 81/350\n",
      "Train Loss 2.6174 | Acc 0.5767\n",
      "Val   Loss 2.1811 | Acc 0.6141\n",
      "\n",
      "Epoch 82/350\n",
      "Train Loss 2.4545 | Acc 0.6280\n",
      "Val   Loss 2.4278 | Acc 0.5352\n",
      "\n",
      "Epoch 83/350\n",
      "Train Loss 2.5866 | Acc 0.5917\n",
      "Val   Loss 2.4463 | Acc 0.5310\n",
      "\n",
      "Epoch 84/350\n",
      "Train Loss 2.4887 | Acc 0.6227\n",
      "Val   Loss 2.2254 | Acc 0.5944\n",
      "\n",
      "Epoch 85/350\n",
      "Train Loss 2.4583 | Acc 0.6255\n",
      "Val   Loss 2.2812 | Acc 0.5817\n",
      "\n",
      "Epoch 86/350\n",
      "Train Loss 2.6915 | Acc 0.5663\n",
      "Val   Loss 2.2558 | Acc 0.5915\n",
      "\n",
      "Epoch 87/350\n",
      "Train Loss 2.3311 | Acc 0.6512\n",
      "Val   Loss 2.2177 | Acc 0.5944\n",
      "\n",
      "Epoch 88/350\n",
      "Train Loss 2.4310 | Acc 0.6333\n",
      "Val   Loss 2.7026 | Acc 0.4465\n",
      "\n",
      "Epoch 89/350\n",
      "Train Loss 2.5752 | Acc 0.6005\n",
      "Val   Loss 2.1603 | Acc 0.5915\n",
      "\n",
      "Epoch 90/350\n",
      "Train Loss 2.4872 | Acc 0.6211\n",
      "Val   Loss 2.6915 | Acc 0.4451\n",
      "\n",
      "Epoch 91/350\n",
      "Train Loss 2.5530 | Acc 0.6124\n",
      "Val   Loss 2.3453 | Acc 0.5479\n",
      "\n",
      "Epoch 92/350\n",
      "Train Loss 2.3858 | Acc 0.6482\n",
      "Val   Loss 2.4736 | Acc 0.5000\n",
      "\n",
      "Epoch 93/350\n",
      "Train Loss 2.4910 | Acc 0.6200\n",
      "Val   Loss 2.7573 | Acc 0.4366\n",
      "\n",
      "Epoch 94/350\n",
      "Train Loss 2.4626 | Acc 0.6320\n",
      "Val   Loss 2.1028 | Acc 0.6423\n",
      "\n",
      "Epoch 95/350\n",
      "Train Loss 2.2993 | Acc 0.6816\n",
      "Val   Loss 1.9769 | Acc 0.6662\n",
      "\n",
      "Epoch 96/350\n",
      "Train Loss 2.3712 | Acc 0.6606\n",
      "Val   Loss 2.2161 | Acc 0.5775\n",
      "\n",
      "Epoch 97/350\n",
      "Train Loss 2.4971 | Acc 0.6257\n",
      "Val   Loss 2.3063 | Acc 0.5704\n",
      "\n",
      "Epoch 98/350\n",
      "Train Loss 2.3341 | Acc 0.6738\n",
      "Val   Loss 2.2740 | Acc 0.5831\n",
      "\n",
      "Epoch 99/350\n",
      "Train Loss 2.2570 | Acc 0.6958\n",
      "Val   Loss 2.2415 | Acc 0.5775\n",
      "\n",
      "Epoch 100/350\n",
      "Train Loss 2.2414 | Acc 0.7035\n",
      "Val   Loss 2.0243 | Acc 0.6563\n",
      "\n",
      "Epoch 101/350\n",
      "Train Loss 2.3488 | Acc 0.6666\n",
      "Val   Loss 2.2840 | Acc 0.5831\n",
      "\n",
      "Epoch 102/350\n",
      "Train Loss 2.2906 | Acc 0.6875\n",
      "Val   Loss 2.1269 | Acc 0.6296\n",
      "\n",
      "Epoch 103/350\n",
      "Train Loss 2.4280 | Acc 0.6494\n",
      "Val   Loss 2.3483 | Acc 0.5380\n",
      "\n",
      "Epoch 104/350\n",
      "Train Loss 2.3297 | Acc 0.6796\n",
      "Val   Loss 2.1520 | Acc 0.6056\n",
      "\n",
      "Epoch 105/350\n",
      "Train Loss 2.1915 | Acc 0.7139\n",
      "Val   Loss 1.8998 | Acc 0.7070\n",
      "\n",
      "Epoch 106/350\n",
      "Train Loss 2.2493 | Acc 0.7060\n",
      "Val   Loss 2.1067 | Acc 0.6338\n",
      "\n",
      "Epoch 107/350\n",
      "Train Loss 2.4455 | Acc 0.6517\n",
      "Val   Loss 2.2155 | Acc 0.5831\n",
      "\n",
      "Epoch 108/350\n",
      "Train Loss 2.3242 | Acc 0.6816\n",
      "Val   Loss 2.3547 | Acc 0.5563\n",
      "\n",
      "Epoch 109/350\n",
      "Train Loss 2.2161 | Acc 0.7027\n",
      "Val   Loss 2.0854 | Acc 0.6437\n",
      "\n",
      "Epoch 110/350\n",
      "Train Loss 2.2293 | Acc 0.7166\n",
      "Val   Loss 2.1130 | Acc 0.6366\n",
      "\n",
      "Epoch 111/350\n",
      "Train Loss 2.3048 | Acc 0.6919\n",
      "Val   Loss 1.8164 | Acc 0.7352\n",
      "Saved best model\n",
      "\n",
      "Epoch 112/350\n",
      "Train Loss 2.2543 | Acc 0.7100\n",
      "Val   Loss 1.7835 | Acc 0.7507\n",
      "Saved best model\n",
      "\n",
      "Epoch 113/350\n",
      "Train Loss 1.9993 | Acc 0.7649\n",
      "Val   Loss 2.0219 | Acc 0.6338\n",
      "\n",
      "Epoch 114/350\n",
      "Train Loss 2.0670 | Acc 0.7597\n",
      "Val   Loss 1.9046 | Acc 0.6930\n",
      "\n",
      "Epoch 115/350\n",
      "Train Loss 2.1771 | Acc 0.7303\n",
      "Val   Loss 1.7886 | Acc 0.7352\n",
      "\n",
      "Epoch 116/350\n",
      "Train Loss 2.0588 | Acc 0.7544\n",
      "Val   Loss 1.8854 | Acc 0.7155\n",
      "\n",
      "Epoch 117/350\n",
      "Train Loss 2.0057 | Acc 0.7801\n",
      "Val   Loss 1.8872 | Acc 0.6859\n",
      "\n",
      "Epoch 118/350\n",
      "Train Loss 2.2489 | Acc 0.7027\n",
      "Val   Loss 2.3450 | Acc 0.5761\n",
      "\n",
      "Epoch 119/350\n",
      "Train Loss 2.3118 | Acc 0.6972\n",
      "Val   Loss 1.9727 | Acc 0.6873\n",
      "\n",
      "Epoch 120/350\n",
      "Train Loss 2.1557 | Acc 0.7371\n",
      "Val   Loss 1.8143 | Acc 0.7324\n",
      "\n",
      "Epoch 121/350\n",
      "Train Loss 2.2223 | Acc 0.7107\n",
      "Val   Loss 1.7890 | Acc 0.7268\n",
      "\n",
      "Epoch 122/350\n",
      "Train Loss 2.0542 | Acc 0.7623\n",
      "Val   Loss 1.7780 | Acc 0.7211\n",
      "\n",
      "Epoch 123/350\n",
      "Train Loss 2.1843 | Acc 0.7195\n",
      "Val   Loss 1.7853 | Acc 0.7423\n",
      "\n",
      "Epoch 124/350\n",
      "Train Loss 2.2647 | Acc 0.7148\n",
      "Val   Loss 2.0561 | Acc 0.6662\n",
      "\n",
      "Epoch 125/350\n",
      "Train Loss 2.0186 | Acc 0.7749\n",
      "Val   Loss 1.9076 | Acc 0.7014\n",
      "\n",
      "Epoch 126/350\n",
      "Train Loss 2.0859 | Acc 0.7526\n",
      "Val   Loss 1.7053 | Acc 0.7451\n",
      "\n",
      "Epoch 127/350\n",
      "Train Loss 2.1362 | Acc 0.7498\n",
      "Val   Loss 1.7447 | Acc 0.7465\n",
      "\n",
      "Epoch 128/350\n",
      "Train Loss 1.9818 | Acc 0.7845\n",
      "Val   Loss 1.6562 | Acc 0.7831\n",
      "Saved best model\n",
      "\n",
      "Epoch 129/350\n",
      "Train Loss 2.2519 | Acc 0.7182\n",
      "Val   Loss 1.7394 | Acc 0.7577\n",
      "\n",
      "Epoch 130/350\n",
      "Train Loss 2.0408 | Acc 0.7747\n",
      "Val   Loss 1.7258 | Acc 0.7648\n",
      "\n",
      "Epoch 131/350\n",
      "Train Loss 2.1376 | Acc 0.7517\n",
      "Val   Loss 1.7640 | Acc 0.7437\n",
      "\n",
      "Epoch 132/350\n",
      "Train Loss 2.1252 | Acc 0.7579\n",
      "Val   Loss 1.6850 | Acc 0.7690\n",
      "\n",
      "Epoch 133/350\n",
      "Train Loss 2.2319 | Acc 0.7318\n",
      "Val   Loss 1.7350 | Acc 0.7563\n",
      "\n",
      "Epoch 134/350\n",
      "Train Loss 2.1559 | Acc 0.7464\n",
      "Val   Loss 1.7033 | Acc 0.7690\n",
      "\n",
      "Epoch 135/350\n",
      "Train Loss 2.0730 | Acc 0.7693\n",
      "Val   Loss 1.7236 | Acc 0.7479\n",
      "\n",
      "Epoch 136/350\n",
      "Train Loss 1.9643 | Acc 0.7911\n",
      "Val   Loss 1.6433 | Acc 0.7831\n",
      "\n",
      "Epoch 137/350\n",
      "Train Loss 2.1046 | Acc 0.7573\n",
      "Val   Loss 1.7348 | Acc 0.7535\n",
      "\n",
      "Epoch 138/350\n",
      "Train Loss 1.9944 | Acc 0.7930\n",
      "Val   Loss 1.6387 | Acc 0.7817\n",
      "\n",
      "Epoch 139/350\n",
      "Train Loss 2.0197 | Acc 0.7800\n",
      "Val   Loss 1.7059 | Acc 0.7690\n",
      "\n",
      "Epoch 140/350\n",
      "Train Loss 2.0870 | Acc 0.7685\n",
      "Val   Loss 1.6672 | Acc 0.7831\n",
      "\n",
      "Epoch 141/350\n",
      "Train Loss 2.1374 | Acc 0.7484\n",
      "Val   Loss 1.6359 | Acc 0.7915\n",
      "Saved best model\n",
      "\n",
      "Epoch 142/350\n",
      "Train Loss 2.1433 | Acc 0.7478\n",
      "Val   Loss 1.6513 | Acc 0.7887\n",
      "\n",
      "Epoch 143/350\n",
      "Train Loss 2.1534 | Acc 0.7503\n",
      "Val   Loss 1.6712 | Acc 0.7718\n",
      "\n",
      "Epoch 144/350\n",
      "Train Loss 2.0660 | Acc 0.7722\n",
      "Val   Loss 1.5993 | Acc 0.8070\n",
      "Saved best model\n",
      "\n",
      "Epoch 145/350\n",
      "Train Loss 2.1112 | Acc 0.7518\n",
      "Val   Loss 1.6264 | Acc 0.7859\n",
      "\n",
      "Epoch 146/350\n",
      "Train Loss 2.0858 | Acc 0.7608\n",
      "Val   Loss 1.6170 | Acc 0.7944\n",
      "\n",
      "Epoch 147/350\n",
      "Train Loss 2.0740 | Acc 0.7696\n",
      "Val   Loss 1.6280 | Acc 0.8000\n",
      "\n",
      "Epoch 148/350\n",
      "Train Loss 2.0725 | Acc 0.7668\n",
      "Val   Loss 1.6266 | Acc 0.8056\n",
      "\n",
      "Epoch 149/350\n",
      "Train Loss 1.9601 | Acc 0.7999\n",
      "Val   Loss 1.6301 | Acc 0.7930\n",
      "\n",
      "Epoch 150/350\n",
      "Train Loss 2.1824 | Acc 0.7395\n",
      "Val   Loss 1.6225 | Acc 0.8014\n",
      "\n",
      "Epoch 151/350\n",
      "Train Loss 1.9252 | Acc 0.8074\n",
      "Val   Loss 1.6115 | Acc 0.7972\n",
      "\n",
      "Epoch 152/350\n",
      "Train Loss 2.0252 | Acc 0.7803\n",
      "Val   Loss 1.6143 | Acc 0.7930\n",
      "\n",
      "Epoch 153/350\n",
      "Train Loss 1.9828 | Acc 0.7837\n",
      "Val   Loss 1.6117 | Acc 0.7972\n",
      "\n",
      "Epoch 154/350\n",
      "Train Loss 2.0291 | Acc 0.7741\n",
      "Val   Loss 1.5914 | Acc 0.7972\n",
      "\n",
      "Epoch 155/350\n",
      "Train Loss 2.0336 | Acc 0.7865\n",
      "Val   Loss 1.5876 | Acc 0.8000\n",
      "\n",
      "Epoch 156/350\n",
      "Train Loss 2.0526 | Acc 0.7737\n",
      "Val   Loss 1.6021 | Acc 0.7944\n",
      "\n",
      "Epoch 157/350\n",
      "Train Loss 2.1277 | Acc 0.7607\n",
      "Val   Loss 1.6129 | Acc 0.7944\n",
      "\n",
      "Epoch 158/350\n",
      "Train Loss 2.1098 | Acc 0.7603\n",
      "Val   Loss 1.5973 | Acc 0.7972\n",
      "\n",
      "Epoch 159/350\n",
      "Train Loss 1.9204 | Acc 0.8108\n",
      "Val   Loss 1.5857 | Acc 0.7986\n",
      "\n",
      "Epoch 160/350\n",
      "Train Loss 2.1450 | Acc 0.7510\n",
      "Val   Loss 1.5887 | Acc 0.7901\n",
      "\n",
      "Epoch 161/350\n",
      "Train Loss 1.9791 | Acc 0.7982\n",
      "Val   Loss 1.5969 | Acc 0.7986\n",
      "\n",
      "Epoch 162/350\n",
      "Train Loss 1.9454 | Acc 0.7974\n",
      "Val   Loss 1.5832 | Acc 0.8028\n",
      "\n",
      "Epoch 163/350\n",
      "Train Loss 2.0268 | Acc 0.7902\n",
      "Val   Loss 1.5969 | Acc 0.7930\n",
      "\n",
      "Epoch 164/350\n",
      "Train Loss 2.1851 | Acc 0.7467\n",
      "Val   Loss 1.5959 | Acc 0.7901\n",
      "\n",
      "Epoch 165/350\n",
      "Train Loss 2.1396 | Acc 0.7608\n",
      "Val   Loss 1.5884 | Acc 0.7958\n",
      "\n",
      "Epoch 166/350\n",
      "Train Loss 2.1535 | Acc 0.7495\n",
      "Val   Loss 1.5863 | Acc 0.7958\n",
      "\n",
      "Epoch 167/350\n",
      "Train Loss 2.0346 | Acc 0.7794\n",
      "Val   Loss 1.5847 | Acc 0.8014\n",
      "\n",
      "Epoch 168/350\n",
      "Train Loss 2.1211 | Acc 0.7505\n",
      "Val   Loss 1.5856 | Acc 0.8014\n",
      "\n",
      "Epoch 169/350\n",
      "Train Loss 1.9919 | Acc 0.7907\n",
      "Val   Loss 1.5741 | Acc 0.8028\n",
      "\n",
      "Epoch 170/350\n",
      "Train Loss 1.9738 | Acc 0.7906\n",
      "Val   Loss 1.5787 | Acc 0.8014\n",
      "\n",
      "Epoch 171/350\n",
      "Train Loss 1.8906 | Acc 0.8191\n",
      "Val   Loss 1.5785 | Acc 0.8000\n",
      "\n",
      "Epoch 172/350\n",
      "Train Loss 2.0355 | Acc 0.7758\n",
      "Val   Loss 1.5817 | Acc 0.7972\n",
      "\n",
      "Epoch 173/350\n",
      "Train Loss 2.0698 | Acc 0.7674\n",
      "Val   Loss 1.5853 | Acc 0.8028\n",
      "\n",
      "Epoch 174/350\n",
      "Train Loss 2.1265 | Acc 0.7610\n",
      "Val   Loss 1.5760 | Acc 0.7972\n",
      "\n",
      "Epoch 175/350\n",
      "Train Loss 1.9896 | Acc 0.7932\n",
      "Val   Loss 1.5768 | Acc 0.8014\n",
      "\n",
      "Epoch 176/350\n",
      "Train Loss 2.2534 | Acc 0.7239\n",
      "Val   Loss 2.8075 | Acc 0.4662\n",
      "\n",
      "Epoch 177/350\n",
      "Train Loss 2.2858 | Acc 0.7050\n",
      "Val   Loss 2.3192 | Acc 0.5972\n",
      "\n",
      "Epoch 178/350\n",
      "Train Loss 2.0696 | Acc 0.7606\n",
      "Val   Loss 2.0717 | Acc 0.6620\n",
      "\n",
      "Epoch 179/350\n",
      "Train Loss 2.0994 | Acc 0.7535\n",
      "Val   Loss 2.0281 | Acc 0.6451\n",
      "\n",
      "Epoch 180/350\n",
      "Train Loss 2.1239 | Acc 0.7402\n",
      "Val   Loss 2.1512 | Acc 0.6254\n",
      "\n",
      "Epoch 181/350\n",
      "Train Loss 2.1294 | Acc 0.7467\n",
      "Val   Loss 2.1166 | Acc 0.6239\n",
      "\n",
      "Epoch 182/350\n",
      "Train Loss 2.1081 | Acc 0.7479\n",
      "Val   Loss 2.6422 | Acc 0.4873\n",
      "\n",
      "Epoch 183/350\n",
      "Train Loss 2.1885 | Acc 0.7278\n",
      "Val   Loss 1.8376 | Acc 0.7296\n",
      "\n",
      "Epoch 184/350\n",
      "Train Loss 2.0353 | Acc 0.7713\n",
      "Val   Loss 1.9042 | Acc 0.6958\n",
      "Early stopping\n",
      "\n",
      "FINAL TEST ACC: 0.819377990430622\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATA_DIR = Path(r\"E:\\ASL_Citizen\\NEW\\preprocessed_approach1_shahd\")\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 350\n",
    "LR = 2e-4\n",
    "PATIENCE = 40\n",
    "MIN_DELTA = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "MIXUP_ALPHA = 0.3\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# =========================\n",
    "# BUILD DATA LIST FROM FILES\n",
    "# =========================\n",
    "files = sorted(DATA_DIR.glob(\"*.npy\"))\n",
    "\n",
    "samples = []\n",
    "labels = []\n",
    "\n",
    "for f in files:\n",
    "    name = f.stem                # \"ABOUT 4\" or \"ABOUT\"\n",
    "    word = name.split(\" \")[0]    # take first token\n",
    "    samples.append(str(f))\n",
    "    labels.append(word)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "encoded = le.fit_transform(labels)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "print(\"Total samples:\", len(samples))\n",
    "print(\"Classes:\", num_classes)\n",
    "\n",
    "# Create dataframe-like lists\n",
    "data = list(zip(samples, encoded))\n",
    "\n",
    "# =========================\n",
    "# SPLITS\n",
    "# =========================\n",
    "train_data, test_data = train_test_split(\n",
    "    data, test_size=0.15, stratify=encoded, random_state=SEED\n",
    ")\n",
    "\n",
    "train_labels = [y for _,y in train_data]\n",
    "train_data, val_data = train_test_split(\n",
    "    train_data, test_size=0.15,\n",
    "    stratify=train_labels,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class ASLDataset(Dataset):\n",
    "    def __init__(self, data, train=True):\n",
    "        self.data = data\n",
    "        self.train = train\n",
    "\n",
    "    def augment(self, x):\n",
    "        if random.random() < 0.5:\n",
    "            shift = random.randint(-5,5)\n",
    "            x = np.roll(x, shift, axis=0)\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            x += np.random.normal(0,0.015,x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.data[idx]\n",
    "        x = np.load(path)\n",
    "\n",
    "        # flatten if (frames, landmarks, 2)\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = x.astype(np.float32)\n",
    "\n",
    "        if self.train:\n",
    "            x = self.augment(x)\n",
    "\n",
    "        return torch.tensor(x), torch.tensor(label)\n",
    "\n",
    "train_loader = DataLoader(ASLDataset(train_data,True), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(ASLDataset(val_data,False), batch_size=BATCH_SIZE)\n",
    "test_loader  = DataLoader(ASLDataset(test_data,False), batch_size=BATCH_SIZE)\n",
    "\n",
    "# =========================\n",
    "# AUTO DETECT INPUT DIM\n",
    "# =========================\n",
    "sample = np.load(samples[0])\n",
    "if len(sample.shape)==3:\n",
    "    input_dim = sample.shape[1]*sample.shape[2]\n",
    "else:\n",
    "    input_dim = sample.shape[1]\n",
    "\n",
    "print(\"Input dim:\", input_dim)\n",
    "\n",
    "# =========================\n",
    "# CLASS WEIGHTS\n",
    "# =========================\n",
    "train_labels = [y for _,y in train_data]\n",
    "counts = np.bincount(train_labels)\n",
    "weights = torch.tensor(1.0/counts,dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights,label_smoothing=0.1)\n",
    "\n",
    "# =========================\n",
    "# MODEL (UNCHANGED)\n",
    "# =========================\n",
    "class ASLModel(nn.Module):\n",
    "    def __init__(self,input_dim=input_dim,num_classes=num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_dim,192,5,padding=2)\n",
    "        self.conv2 = nn.Conv1d(192,192,3,padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.bn2 = nn.BatchNorm1d(192)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            192,192,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(384,128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(384,192),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(192,num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.dropout(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.transpose(1,2)\n",
    "\n",
    "        lstm_out,_ = self.lstm(x)\n",
    "\n",
    "        attn = self.attention(lstm_out)\n",
    "        weights = torch.softmax(attn,dim=1)\n",
    "        pooled = torch.sum(weights*lstm_out,dim=1)\n",
    "\n",
    "        return self.fc(pooled)\n",
    "\n",
    "model = ASLModel().to(DEVICE)\n",
    "\n",
    "# =========================\n",
    "# OPTIMIZER & SCHEDULER\n",
    "# =========================\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=LR,weight_decay=1e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=25,\n",
    "    T_mult=2,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# TRAIN LOOP (UNCHANGED)\n",
    "# =========================\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "    train_correct=0\n",
    "    total=0\n",
    "\n",
    "    for x,y in train_loader:\n",
    "        x,y=x.to(DEVICE),y.to(DEVICE)\n",
    "\n",
    "        lam = np.random.beta(MIXUP_ALPHA,MIXUP_ALPHA)\n",
    "        perm = torch.randperm(x.size(0)).to(DEVICE)\n",
    "        x_mix = lam*x + (1-lam)*x[perm]\n",
    "        y_a,y_b=y,y[perm]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(x_mix)\n",
    "\n",
    "        loss = lam*criterion(outputs,y_a)+(1-lam)*criterion(outputs,y_b)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss+=loss.item()*y.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "\n",
    "        train_correct += (\n",
    "            lam * (preds == y_a).sum().item() +\n",
    "            (1 - lam) * (preds == y_b).sum().item()\n",
    "        )\n",
    "        total+=y.size(0)\n",
    "\n",
    "    train_loss/=total\n",
    "    train_acc=train_correct/total\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    val_loss=0\n",
    "    val_correct=0\n",
    "    total_val=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_loader:\n",
    "            x,y=x.to(DEVICE),y.to(DEVICE)\n",
    "            outputs=model(x)\n",
    "            loss=criterion(outputs,y)\n",
    "\n",
    "            val_loss+=loss.item()*y.size(0)\n",
    "            preds=outputs.argmax(1)\n",
    "            val_correct+=(preds==y).sum().item()\n",
    "            total_val+=y.size(0)\n",
    "\n",
    "    val_loss/=total_val\n",
    "    val_acc=val_correct/total_val\n",
    "\n",
    "    scheduler.step(epoch + val_loss)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss {train_loss:.4f} | Acc {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss {val_loss:.4f} | Acc {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc + MIN_DELTA:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(),\"best_asl_model.pth\")\n",
    "        print(\"Saved best model\")\n",
    "    else:\n",
    "        patience_counter+=1\n",
    "        if patience_counter>=PATIENCE:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# =========================\n",
    "# TEST\n",
    "# =========================\n",
    "model.load_state_dict(torch.load(\"best_asl_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "correct=0\n",
    "total=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x,y in test_loader:\n",
    "        x,y=x.to(DEVICE),y.to(DEVICE)\n",
    "        preds=model(x).argmax(1)\n",
    "        correct+=(preds==y).sum().item()\n",
    "        total+=y.size(0)\n",
    "\n",
    "print(\"\\nFINAL TEST ACC:\",correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86c575-4abb-4a6a-b236-276b175297e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
