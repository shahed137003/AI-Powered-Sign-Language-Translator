{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9255418c-856a-4caf-9827-51ba96a2087e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Enhanced Sign Language Preprocessing (No Sliding Window, No Masks)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning ASL_CITIZEN_DIR: 100%|████████████████████████████████████████████████| 5568/5568 [00:00<00:00, 144263.08it/s]\n",
      "Processing Videos: 100%|███████████████████████████████████████████████████████████| 5568/5568 [11:34<00:00,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! Processed files are saved in E:\\ASL_Citizen\\NEW\\Top_Classes_Landmarks_Preprocessed_method2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    # Paths\n",
    "    ASL_CITIZEN_DIR = Path(r\"E:\\ASL_Citizen\\NEW\\Top_Classes_Landmarks\")  # Single dataset folder\n",
    "    OUTPUT_DIR = Path(r\"E:\\ASL_Citizen\\NEW\\Top_Classes_Landmarks_Preprocessed_method2\")\n",
    "    SPLITS_DIR = Path(\"./data/Enhanced_Splits_157Frames\")\n",
    "    ANALYSIS_DIR = Path(\"./data/analysis_results\")\n",
    "    \n",
    "    # Create directories\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    SPLITS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Based on your analysis\n",
    "    TARGET_FRAMES = 157\n",
    "    FEATURE_DIM = 438\n",
    "    MIN_SAMPLES_PER_WORD = 5\n",
    "    \n",
    "    # Frame strategy parameters\n",
    "    MAX_SINGLE_FRAMES = 140\n",
    "    WINDOW_THRESHOLD = 161\n",
    "    VERY_LONG_THRESHOLD = 201\n",
    "    \n",
    "    # Geometry constants\n",
    "    POSE_SIZE = 132\n",
    "    HAND_SIZE = 63\n",
    "    FACE_SIZE = 180\n",
    "    POSE_LANDMARKS, POSE_VALS = 33, 4\n",
    "    HAND_LANDMARKS, HAND_VALS = 21, 3\n",
    "    FACE_LANDMARKS, FACE_VALS = 60, 3\n",
    "    LEG_IDXS = list(range(25, 33))\n",
    "    CRITICAL_POSE_IDXS = {0, 11, 12, 13, 14, 15, 16, 23, 24}\n",
    "    \n",
    "    # Preprocessing parameters\n",
    "    SMOOTH_POSE = True\n",
    "    SMOOTH_HANDS = True\n",
    "    SMOOTH_FACE = False\n",
    "    POSE_MIN_CUTOFF = 1.5\n",
    "    POSE_BETA = 0.4\n",
    "    HAND_MIN_CUTOFF = 2.0\n",
    "    HAND_BETA = 0.3\n",
    "    FACE_MIN_CUTOFF = 2.0\n",
    "    FACE_BETA = 0.4\n",
    "    D_CUTOFF = 1.0\n",
    "    FPS = 20.0\n",
    "    EPS = 1e-8\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ============================================================================\n",
    "# GEOMETRY FUNCTIONS\n",
    "# ============================================================================\n",
    "def in_unit_xy(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    return np.isfinite(x) & np.isfinite(y) & (x >= 0.0) & (x <= 1.0) & (y >= 0.0) & (y <= 1.0)\n",
    "\n",
    "def reasonable_xy(x: np.ndarray, y: np.ndarray, lo: float = -0.25, hi: float = 1.25) -> np.ndarray:\n",
    "    return np.isfinite(x) & np.isfinite(y) & (x >= lo) & (x <= hi) & (y >= lo) & (y <= hi)\n",
    "\n",
    "def valid_points_xyz(arr: np.ndarray, eps: float = 1e-8) -> np.ndarray:\n",
    "    return np.any(np.abs(arr) > eps, axis=-1)\n",
    "\n",
    "def is_valid_wrist(w: np.ndarray, eps: float = 1e-8) -> bool:\n",
    "    return bool(np.isfinite(w).all() and np.any(np.abs(w) > eps))\n",
    "\n",
    "def dist2(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.linalg.norm(a[:2] - b[:2]))\n",
    "\n",
    "# ============================================================================\n",
    "# NORMALIZATION\n",
    "# ============================================================================\n",
    "def compute_global_root(pose_xyz: np.ndarray, vis: np.ndarray, eps: float = 1e-8) -> np.ndarray:\n",
    "    def collect_mid(i1: int, i2: int):\n",
    "        m = (vis[:, i1] > 0.0) & (vis[:, i2] > 0.0)\n",
    "        m = m & valid_points_xyz(pose_xyz[:, i1, :], eps) & valid_points_xyz(pose_xyz[:, i2, :], eps)\n",
    "        if not np.any(m):\n",
    "            return None\n",
    "        return (pose_xyz[m, i1, :] + pose_xyz[m, i2, :]) / 2.0\n",
    "\n",
    "    mid_hip = collect_mid(23, 24)\n",
    "    if mid_hip is not None:\n",
    "        return mid_hip.mean(axis=0)\n",
    "\n",
    "    mid_sh = collect_mid(11, 12)\n",
    "    if mid_sh is not None:\n",
    "        return mid_sh.mean(axis=0)\n",
    "\n",
    "    m_nose = (vis[:, 0] > 0.0) & valid_points_xyz(pose_xyz[:, 0, :], eps)\n",
    "    if np.any(m_nose):\n",
    "        return pose_xyz[m_nose, 0, :].mean(axis=0)\n",
    "\n",
    "    m_all = (vis > 0.0) & valid_points_xyz(pose_xyz, eps)\n",
    "    if np.any(m_all):\n",
    "        return pose_xyz[m_all].mean(axis=0)\n",
    "\n",
    "    return np.zeros(3, dtype=np.float32)\n",
    "\n",
    "def compute_global_scale(pose_xyz: np.ndarray, vis: np.ndarray, root: np.ndarray, eps: float = 1e-6) -> float:\n",
    "    def collect_dist(i1: int, i2: int):\n",
    "        m = (vis[:, i1] > 0.0) & (vis[:, i2] > 0.0)\n",
    "        m = m & valid_points_xyz(pose_xyz[:, i1, :]) & valid_points_xyz(pose_xyz[:, i2, :])\n",
    "        if not np.any(m):\n",
    "            return None\n",
    "        d = np.linalg.norm(pose_xyz[m, i1, :] - pose_xyz[m, i2, :], axis=1)\n",
    "        d = d[d > eps]\n",
    "        return d if d.size > 0 else None\n",
    "\n",
    "    d_sh = collect_dist(11, 12)\n",
    "    if d_sh is not None:\n",
    "        return float(d_sh.mean())\n",
    "\n",
    "    d_hip = collect_dist(23, 24)\n",
    "    if d_hip is not None:\n",
    "        return float(d_hip.mean())\n",
    "\n",
    "    m_all = (vis > 0.0) & valid_points_xyz(pose_xyz)\n",
    "    if np.any(m_all):\n",
    "        d = np.linalg.norm(pose_xyz[m_all] - root[None, :], axis=1)\n",
    "        d = d[d > eps]\n",
    "        if d.size > 0:\n",
    "            return float(d.mean())\n",
    "\n",
    "    return 1.0\n",
    "\n",
    "# ============================================================================\n",
    "# HAND FIXING\n",
    "# ============================================================================\n",
    "def frame_valid_hand(hand_t: np.ndarray, min_pts: int = 8, eps: float = 1e-8) -> bool:\n",
    "    nz = np.any(np.abs(hand_t) > eps, axis=1)\n",
    "    return int(nz.sum()) >= int(min_pts)\n",
    "\n",
    "def hand_centroid(hand_t: np.ndarray, eps: float = 1e-8):\n",
    "    m = np.any(np.abs(hand_t) > eps, axis=1)\n",
    "    if not np.any(m):\n",
    "        return None\n",
    "    return hand_t[m].mean(axis=0)\n",
    "\n",
    "def fix_swap_and_gate_hands(lh, rh, lw, rw, min_pts=8, hand_wrist_max_dist=1.1, eps=1e-8):\n",
    "    T = lh.shape[0]\n",
    "    for t in range(T):\n",
    "        l_ok = frame_valid_hand(lh[t], min_pts=min_pts, eps=eps)\n",
    "        r_ok = frame_valid_hand(rh[t], min_pts=min_pts, eps=eps)\n",
    "        wl_ok = is_valid_wrist(lw[t], eps=eps)\n",
    "        wr_ok = is_valid_wrist(rw[t], eps=eps)\n",
    "        cL = hand_centroid(lh[t], eps=eps) if l_ok else None\n",
    "        cR = hand_centroid(rh[t], eps=eps) if r_ok else None\n",
    "\n",
    "        if l_ok and r_ok and wl_ok and wr_ok and (cL is not None) and (cR is not None):\n",
    "            d_ll = dist2(cL, lw[t])\n",
    "            d_lr = dist2(cL, rw[t])\n",
    "            d_rr = dist2(cR, rw[t])\n",
    "            d_rl = dist2(cR, lw[t])\n",
    "            if (d_lr + d_rl) + 1e-6 < (d_ll + d_rr):\n",
    "                lh[t], rh[t] = rh[t].copy(), lh[t].copy()\n",
    "                cL, cR = cR, cL\n",
    "\n",
    "        if wl_ok and l_ok and (cL is not None):\n",
    "            if dist2(cL, lw[t]) > hand_wrist_max_dist:\n",
    "                lh[t] = 0.0\n",
    "        if wr_ok and r_ok and (cR is not None):\n",
    "            if dist2(cR, rw[t]) > hand_wrist_max_dist:\n",
    "                rh[t] = 0.0\n",
    "\n",
    "def fill_hand_gaps_wrist_relative_tiered(hand, wrist, small_gap=6, medium_gap=15, min_pts=8, rel_change_thresh=0.7, eps=1e-8):\n",
    "    T = hand.shape[0]\n",
    "    valid = np.array([frame_valid_hand(hand[t], min_pts=min_pts, eps=eps) for t in range(T)], dtype=bool)\n",
    "    idx = np.where(valid)[0]\n",
    "    if idx.size == 0:\n",
    "        return\n",
    "\n",
    "    def set_from_rel(t: int, rel: np.ndarray):\n",
    "        if is_valid_wrist(wrist[t], eps=eps):\n",
    "            hand[t] = rel + wrist[t]\n",
    "\n",
    "    for a, b in zip(idx[:-1], idx[1:]):\n",
    "        gap = int(b - a - 1)\n",
    "        if gap <= 0:\n",
    "            continue\n",
    "        if gap > medium_gap:\n",
    "            continue\n",
    "\n",
    "        if not (is_valid_wrist(wrist[a], eps=eps) and is_valid_wrist(wrist[b], eps=eps)):\n",
    "            if gap <= small_gap:\n",
    "                for t in range(a + 1, b):\n",
    "                    hand[t] = hand[a]\n",
    "            continue\n",
    "\n",
    "        rel_a = hand[a] - wrist[a]\n",
    "        rel_b = hand[b] - wrist[b]\n",
    "\n",
    "        if gap > small_gap:\n",
    "            for t in range(a + 1, b):\n",
    "                set_from_rel(t, rel_a)\n",
    "            continue\n",
    "\n",
    "        delta = np.linalg.norm(rel_a - rel_b, axis=1)\n",
    "        delta = delta[np.isfinite(delta)]\n",
    "        rel_delta = float(np.median(delta)) if delta.size else 999.0\n",
    "\n",
    "        if rel_delta <= rel_change_thresh:\n",
    "            for t in range(a + 1, b):\n",
    "                alpha = (t - a) / (b - a)\n",
    "                rel = (1.0 - alpha) * rel_a + alpha * rel_b\n",
    "                set_from_rel(t, rel)\n",
    "        else:\n",
    "            for t in range(a + 1, b):\n",
    "                set_from_rel(t, rel_a)\n",
    "\n",
    "# ============================================================================\n",
    "# SMOOTHING\n",
    "# ============================================================================\n",
    "def _alpha(cutoff_hz: float, dt: float) -> float:\n",
    "    cutoff_hz = float(max(cutoff_hz, 1e-6))\n",
    "    tau = 1.0 / (2.0 * np.pi * cutoff_hz)\n",
    "    return float(1.0 / (1.0 + tau / dt))\n",
    "\n",
    "def one_euro_filter_series(x: np.ndarray, valid: np.ndarray, fps: float, min_cutoff: float, beta: float, d_cutoff: float) -> np.ndarray:\n",
    "    T, D = x.shape\n",
    "    out = np.zeros_like(x, dtype=np.float32)\n",
    "    dt = 1.0 / float(max(fps, 1e-6))\n",
    "    x_prev = np.zeros(D, dtype=np.float32)\n",
    "    x_hat_prev = np.zeros(D, dtype=np.float32)\n",
    "    dx_hat_prev = np.zeros(D, dtype=np.float32)\n",
    "    has_prev = False\n",
    "\n",
    "    for t in range(T):\n",
    "        if not bool(valid[t]):\n",
    "            has_prev = False\n",
    "            continue\n",
    "        xt = x[t].astype(np.float32, copy=False)\n",
    "        if not has_prev:\n",
    "            out[t] = xt\n",
    "            x_prev = xt\n",
    "            x_hat_prev = xt\n",
    "            dx_hat_prev[:] = 0.0\n",
    "            has_prev = True\n",
    "            continue\n",
    "        dx = (xt - x_prev) / dt\n",
    "        a_d = _alpha(d_cutoff, dt)\n",
    "        dx_hat = a_d * dx + (1.0 - a_d) * dx_hat_prev\n",
    "        cutoff = float(min_cutoff + beta * np.linalg.norm(dx_hat))\n",
    "        a = _alpha(cutoff, dt)\n",
    "        x_hat = a * xt + (1.0 - a) * x_hat_prev\n",
    "        out[t] = x_hat\n",
    "        x_prev = xt\n",
    "        x_hat_prev = x_hat\n",
    "        dx_hat_prev = dx_hat\n",
    "\n",
    "    return out\n",
    "\n",
    "def smooth_points_over_time(pts: np.ndarray, eps: float, fps: float, min_cutoff: float, beta: float, d_cutoff: float) -> None:\n",
    "    T, N, _ = pts.shape\n",
    "    for i in range(N):\n",
    "        x = pts[:, i, :]\n",
    "        valid = valid_points_xyz(x, eps=eps) & np.isfinite(x).all(axis=1)\n",
    "        if not np.any(valid):\n",
    "            continue\n",
    "        pts[:, i, :] = one_euro_filter_series(x, valid=valid, fps=fps, min_cutoff=min_cutoff, beta=beta, d_cutoff=d_cutoff)\n",
    "\n",
    "# ============================================================================\n",
    "# FRAME STRATEGY (SLIDING WINDOW REMOVED)\n",
    "# ============================================================================\n",
    "def adaptive_padding(sequence: np.ndarray, target_frames: int) -> np.ndarray:\n",
    "    T, D = sequence.shape\n",
    "    if T >= target_frames:\n",
    "        return sequence[:target_frames]\n",
    "    padded_seq = np.zeros((target_frames, D), dtype=np.float32)\n",
    "    x_orig = np.arange(T)\n",
    "    x_target = np.linspace(0, T-1, target_frames)\n",
    "    for d in range(D):\n",
    "        if np.any(np.isfinite(sequence[:, d])):\n",
    "            if T >= 2:\n",
    "                f = interpolate.interp1d(x_orig, sequence[:, d], kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n",
    "                padded_seq[:, d] = f(x_target)\n",
    "            else:\n",
    "                padded_seq[:, d] = sequence[0, d]\n",
    "    return padded_seq\n",
    "\n",
    "def hybrid_frame_strategy(sequence: np.ndarray) -> np.ndarray:\n",
    "    T = sequence.shape[0]\n",
    "    if T < config.TARGET_FRAMES:\n",
    "        padded = adaptive_padding(sequence, config.TARGET_FRAMES)\n",
    "        return padded\n",
    "    elif T > config.TARGET_FRAMES:\n",
    "        return sequence[:config.TARGET_FRAMES]\n",
    "    return sequence\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESS SINGLE VIDEO\n",
    "# ============================================================================\n",
    "def preprocess_sequence_global(seq: np.ndarray) -> np.ndarray:\n",
    "    y = seq.astype(np.float32, copy=True)\n",
    "    if y.ndim != 2 or y.shape[1] != config.FEATURE_DIM:\n",
    "        raise ValueError(f\"Expected shape (T,{config.FEATURE_DIM}), got {y.shape}\")\n",
    "    pose = y[:, :config.POSE_SIZE].reshape(-1, config.POSE_LANDMARKS, config.POSE_VALS)\n",
    "    face = y[:, config.POSE_SIZE:config.POSE_SIZE + config.FACE_SIZE].reshape(-1, config.FACE_LANDMARKS, config.FACE_VALS)\n",
    "    lh = y[:, config.POSE_SIZE + config.FACE_SIZE:config.POSE_SIZE + config.FACE_SIZE + config.HAND_SIZE].reshape(-1, config.HAND_LANDMARKS, config.HAND_VALS)\n",
    "    rh = y[:, config.POSE_SIZE + config.FACE_SIZE + config.HAND_SIZE:].reshape(-1, config.HAND_LANDMARKS, config.HAND_VALS)\n",
    "    \n",
    "    # Pose cleaning\n",
    "    px, py, pz, pv = pose[:, :, 0], pose[:, :, 1], pose[:, :, 2], pose[:, :, 3]\n",
    "    finite_pose = np.isfinite(pz) & np.isfinite(pv)\n",
    "    pose_in_strict = in_unit_xy(px, py) & finite_pose\n",
    "    pose_in_relaxed = reasonable_xy(px, py) & finite_pose\n",
    "    critical_mask = np.zeros((pose.shape[0], config.POSE_LANDMARKS), dtype=bool)\n",
    "    for i in config.CRITICAL_POSE_IDXS:\n",
    "        critical_mask[:, i] = True\n",
    "    pose_keep_for_transform = (pv >= 0.1) & pose_in_strict\n",
    "    pose_keep_for_transform = pose_keep_for_transform | (critical_mask & pose_in_relaxed)\n",
    "    pose_keep_visible = (pv >= 0.1) & pose_in_strict\n",
    "    bad_xyz = ~pose_keep_for_transform\n",
    "    pose[bad_xyz, :3] = 0.0\n",
    "    pose[~pose_keep_visible, 3] = 0.0\n",
    "    pose[:, config.LEG_IDXS, :3] = 0.0\n",
    "    pose[:, config.LEG_IDXS, 3] = 0.0\n",
    "\n",
    "    # Face cleaning\n",
    "    fx, fy, fz = face[:, :, 0], face[:, :, 1], face[:, :, 2]\n",
    "    face_in = reasonable_xy(fx, fy) & np.isfinite(fz)\n",
    "    face[~face_in, :3] = 0.0\n",
    "\n",
    "    # Hands cleaning\n",
    "    lx, ly, lz = lh[:, :, 0], lh[:, :, 1], lh[:, :, 2]\n",
    "    lh_in = reasonable_xy(lx, ly) & np.isfinite(lz)\n",
    "    lh[~lh_in, :3] = 0.0\n",
    "    rx, ry, rz = rh[:, :, 0], rh[:, :, 1], rh[:, :, 2]\n",
    "    rh_in = reasonable_xy(rx, ry) & np.isfinite(rz)\n",
    "    rh[~rh_in, :3] = 0.0\n",
    "\n",
    "    # Global normalization\n",
    "    pose_xyz = pose[:, :, :3]\n",
    "    vis = pose[:, :, 3]\n",
    "    root = compute_global_root(pose_xyz, vis, eps=config.EPS)\n",
    "    scale = compute_global_scale(pose_xyz, vis, root)\n",
    "    pose_valid_for_transform = pose_keep_for_transform & valid_points_xyz(pose_xyz, eps=config.EPS)\n",
    "    pose_xyz[pose_valid_for_transform] = (pose_xyz[pose_valid_for_transform] - root) / scale\n",
    "    pose[:, :, :3] = pose_xyz\n",
    "    for arr in (face, lh, rh):\n",
    "        m = valid_points_xyz(arr, eps=config.EPS)\n",
    "        arr[m] = (arr[m] - root) / scale\n",
    "\n",
    "    # Wrist positions\n",
    "    lw = pose_xyz[:, 15, :].copy()\n",
    "    rw = pose_xyz[:, 16, :].copy()\n",
    "\n",
    "    # Hand fixing\n",
    "    fix_swap_and_gate_hands(lh, rh, lw, rw)\n",
    "    fill_hand_gaps_wrist_relative_tiered(lh, lw)\n",
    "    fill_hand_gaps_wrist_relative_tiered(rh, rw)\n",
    "\n",
    "    # Smoothing\n",
    "    if config.SMOOTH_POSE:\n",
    "        smooth_points_over_time(pose[:, :, :3], eps=config.EPS, fps=config.FPS,\n",
    "                                min_cutoff=config.POSE_MIN_CUTOFF, beta=config.POSE_BETA, d_cutoff=config.D_CUTOFF)\n",
    "    if config.SMOOTH_HANDS:\n",
    "        smooth_points_over_time(lh, eps=config.EPS, fps=config.FPS,\n",
    "                                min_cutoff=config.HAND_MIN_CUTOFF, beta=config.HAND_BETA, d_cutoff=config.D_CUTOFF)\n",
    "        smooth_points_over_time(rh, eps=config.EPS, fps=config.FPS,\n",
    "                                min_cutoff=config.HAND_MIN_CUTOFF, beta=config.HAND_BETA, d_cutoff=config.D_CUTOFF)\n",
    "    if config.SMOOTH_FACE:\n",
    "        smooth_points_over_time(face, eps=config.EPS, fps=config.FPS,\n",
    "                                min_cutoff=config.FACE_MIN_CUTOFF, beta=config.FACE_BETA, d_cutoff=config.D_CUTOFF)\n",
    "\n",
    "    # Reconstruct\n",
    "    out = np.empty_like(y, dtype=np.float32)\n",
    "    out[:, :config.POSE_SIZE] = pose.reshape(-1, config.POSE_SIZE)\n",
    "    out[:, config.POSE_SIZE:config.POSE_SIZE + config.FACE_SIZE] = face.reshape(-1, config.FACE_SIZE)\n",
    "    out[:, config.POSE_SIZE + config.FACE_SIZE:config.POSE_SIZE + config.FACE_SIZE + config.HAND_SIZE] = lh.reshape(-1, config.HAND_SIZE)\n",
    "    out[:, config.POSE_SIZE + config.FACE_SIZE + config.HAND_SIZE:] = rh.reshape(-1, config.HAND_SIZE)\n",
    "    return out\n",
    "\n",
    "# ============================================================================\n",
    "# FILE HANDLING\n",
    "# ============================================================================\n",
    "def sanitize_filename(filename: str) -> str:\n",
    "    sanitized = re.sub(r'[^a-zA-Z0-9_.]', '_', filename)\n",
    "    sanitized = re.sub(r'_+', '_', sanitized)\n",
    "    sanitized = sanitized.strip('_')\n",
    "    return sanitized\n",
    "\n",
    "def create_safe_filename(word: str, video_id: str) -> str:\n",
    "    safe_word = sanitize_filename(word)\n",
    "    safe_video_id = sanitize_filename(video_id)\n",
    "    return f\"{safe_word}_{safe_video_id}.npy\"\n",
    "\n",
    "# ============================================================================\n",
    "# PROCESS SINGLE VIDEO\n",
    "# ============================================================================\n",
    "def process_single_video(filepath: str, word: str, video_id: str) -> str:\n",
    "    raw_data = np.load(filepath, allow_pickle=True)\n",
    "    cleaned_data = preprocess_sequence_global(raw_data)\n",
    "    final_seq = hybrid_frame_strategy(cleaned_data)\n",
    "    filename = create_safe_filename(word, video_id)\n",
    "    save_path = config.OUTPUT_DIR / filename\n",
    "    np.save(save_path, final_seq)\n",
    "    return str(save_path)\n",
    "\n",
    "# ============================================================================\n",
    "# SCAN DATASET\n",
    "# ============================================================================\n",
    "def scan_dataset():\n",
    "    data = []\n",
    "    files = list(config.ASL_CITIZEN_DIR.glob(\"*.npy\"))\n",
    "    for f in tqdm(files, desc=\"Scanning ASL_CITIZEN_DIR\"):\n",
    "        filename = f.stem\n",
    "        match = re.match(r\"(.+?)\\s*(\\d+)$\", filename)\n",
    "        if match:\n",
    "            word = match.group(1).lower().strip()\n",
    "            video_id = match.group(2)\n",
    "        else:\n",
    "            word = filename.lower()\n",
    "            video_id = \"unknown\"\n",
    "        data.append({'filepath': str(f), 'word': word, 'video_id': video_id})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ============================================================================\n",
    "# PROCESS ALL VIDEOS\n",
    "# ============================================================================\n",
    "def process_all_videos():\n",
    "    df = scan_dataset()\n",
    "    word_counts = df['word'].value_counts()\n",
    "    valid_words = word_counts[word_counts >= config.MIN_SAMPLES_PER_WORD].index\n",
    "    df_filtered = df[df['word'].isin(valid_words)].copy()\n",
    "    for idx, row in tqdm(df_filtered.iterrows(), total=len(df_filtered), desc=\"Processing Videos\"):\n",
    "        process_single_video(row['filepath'], row['word'], row['video_id'])\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*50)\n",
    "    print(\"Enhanced Sign Language Preprocessing (No Sliding Window, No Masks)\")\n",
    "    print(\"=\"*50)\n",
    "    process_all_videos()\n",
    "    print(f\"\\nDone! Processed files are saved in {config.OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f724fde-5af8-4282-a33f-053866568319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
