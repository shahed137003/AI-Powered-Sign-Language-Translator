# ğŸ¤–âœ¨ AI-Powered Sign Language Translator

> **Bridging the communication gap between the Deaf and hearing communities using Artificial Intelligence**

---

## ğŸ§© Overview  

Millions of deaf and hard-of-hearing individuals face daily communication barriers.  
This project proposes an **AI-powered multilingual platform** that performs **real-time, bidirectional translation** between **sign and spoken/written languages**, supporting **education, healthcare, and accessibility** across platforms.  

Built with cutting-edge technologies in **computer vision**, **speech processing**, and **natural language understanding**, our solution empowers inclusive communication for all.

---

## ğŸš€ Features  

### ğŸ‘ Core Features
- **ğŸ¥ Live Camera Translation** â€” Capture real-time gestures via camera â†’ Translate into text or voice.  
- **ğŸ“¹ Video Upload Translation** â€” Translate pre-recorded or YouTube videos into sign animations.  
- **ğŸŒ Multilingual Support** â€” Supports multiple sign (ASL, ArSL) and spoken (Arabic, English) languages.  
- **ğŸ—£ï¸ Voice-to-Sign Conversion** â€” Convert speech input into 3D avatar sign animations in real time.  

### ğŸ’¡ Advanced & Innovative Features
- **ğŸ’¬ AI Sign Chatbot** â€” Practice signing or chatting in sign language with an intelligent assistant.  
- **ğŸ“¶ Offline Mode** â€” Uses TensorFlow Lite for translation in low-connectivity areas.  
- **â™¿ Accessibility Toolkit** â€” Adjustable sign speed, text-to-speech, haptic notifications, high-contrast modes.  

---

## ğŸ—ï¸ System Architecture  

<p align="center">
  <img src="use Experience.png" alt="System Architecture Diagram" width="800"/>
</p>

> The architecture integrates **gesture recognition**, **speech recognition**, **translation**, and **3D avatar rendering** into one seamless AI pipeline.

**Flow Summary:**
1. ğŸ¥ Input sources (Camera, Mic, Text, Video)
2. ğŸ§© AI Processing (Gesture Recognition, Speech-to-Text, NLP)
3. ğŸ”¤ Translation Engine (Seq2Seq Transformer)
4. ğŸ§ Avatar Rendering & Output (3D animations, text, or voice)
5. ğŸŒ Web Interface built with React & Tailwind CSS

---

## ğŸ§  Methodology  

1. **Data Preprocessing** â€“ Extract 3D hand landmarks using MediaPipe.  
2. **Model Training** â€“ Train CNN/LSTM/Transformer models for gesture recognition.  
3. **Multilingual Mapping** â€“ Map gestures to English/Arabic words and gloss sequences.  
4. **Avatar Animation** â€“ Use Blender/Unity to visualize translated signs.  

---

## ğŸ§° Tools & Frameworks  

| Category | Tools Used |
|-----------|-------------|
| Gesture Recognition | MediaPipe, OpenCV |
| AI Models | TensorFlow, PyTorch, Transformers |
| Speech Recognition | Whisper, Wav2Vec2.0, Google Speech-to-Text |
| Avatar Rendering | Unity, Blender, WebGL |
| Front-End | React, Tailwind CSS |
| Back-End | Python, Flask/Django (optional) |

---

## ğŸ§¾ Datasets  

### ğŸ“˜ Arabic Sign Language (ArSL)
- [Arabic Sign Language Alphabet Dataset (Kaggle)](https://www.kaggle.com/datasets/birafaneimane/arabic-sign-language-alphabet-arsl-dataset)
- [Arabic Sign Language Data](https://www.kaggle.com/datasets/mohamedmostafa23334/arabic-sign-language-data)
- [Augmented ArSL Dataset](https://www.kaggle.com/datasets/sabribelmadoui/arabic-sign-language-augmented-dataset)

### ğŸ“— American Sign Language (ASL)
- [MS-ASL: Microsoft Large Scale Dataset](https://microsoft.github.io/data-for-society/dataset?d=MS-ASL-American-Sign-Language-Dataset)
- [ASL Dataset (Kaggle)](https://www.kaggle.com/datasets/ayuraj/asl-dataset)

---

## ğŸ§­ User Journey  

1. **ğŸ  Home Screen** â€“ Choose between *Live Translate*, *Video Upload*, or *Settings*.  
2. **ğŸ¥ Live Mode** â€“ Sign in front of the camera â†’ Real-time text/voice translation.  
3. **ğŸ™ Voice Mode** â€“ Speak into the microphone â†’ See avatar signing your words.  
4. **ğŸ“¹ Upload Mode** â€“ Upload video â†’ Auto-translate with side-by-side sign output.  
5. **âš™ï¸ Accessibility Panel** â€“ Customize contrast, vibration alerts, and voice speed.

---

## ğŸ”® Future Work  

- ğŸ§© Expand to sentence-level translation with PHOENIX-2014T dataset.  
- ğŸ¤– Integrate with Alexa/Google for voice-to-sign capabilities.  
- ğŸ” Cross-sign translation (e.g., ASL â†” BSL).  
- ğŸ§¤ Support wearable haptic feedback gloves.  
- ğŸ•¶ï¸ Smart glasses integration for live subtitles and translation overlays.  

---

## ğŸ§± Project Vision  

> â€œTo create a scalable, inclusive, and multilingual accessibility platform that uses AI to eliminate communication barriers between Deaf and hearing individuals â€” fostering equality in education, healthcare, and daily life.â€

---

## ğŸ“œ License  

This project is licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details.  

---

