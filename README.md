# ğŸ¤–âœ¨ AI-Powered Sign Language Translator (Skeleton-Based)

> **Bridging the communication gap between the Deaf and hearing communities using Artificial Intelligence**

---

## ğŸ§© Overview  

Millions of deaf and hard-of-hearing individuals face daily communication barriers.  
This project proposes an **AI-powered multilingual platform** that performs **real-time, bidirectional translation** between **sign and spoken/written languages**, supporting **education, healthcare, and accessibility** across platforms.  

Unlike traditional avatar-based systems, our solution uses **AI-driven skeleton motion synthesis**, generating **realistic 2D/3D skeletal gestures** directly from text, speech, or video â€” creating a more lightweight, scalable, and research-focused approach.

---

## ğŸš€ Features  

### ğŸ‘ Core Features
- **ğŸ¥ Live Camera Translation** â€” Capture real-time gestures via camera â†’ Translate into text or voice.  
- **ğŸ“¹ Video Upload Translation** â€” Translate pre-recorded or YouTube videos into skeleton-based sign motion.  
- **ğŸŒ Multilingual Support** â€” Supports multiple sign (ASL, ArSL) and spoken (Arabic, English) languages.  
- **ğŸ—£ï¸ Voice-to-Sign Conversion** â€” Convert speech input into AI-generated skeleton signing in real time.  

### ğŸ’¡ Advanced & Innovative Features
- **ğŸ’¬ AI Sign Chatbot** â€” Chat or practice signing with an intelligent assistant that replies using animated skeleton poses.  
- **ğŸ“¶ Offline Mode** â€” TensorFlow Lite models allow translation in low-connectivity environments.  
- **â™¿ Accessibility Toolkit** â€” Adjustable sign speed, vibration feedback, high-contrast visualization, and text-to-speech options.  

---

## ğŸ—ï¸ System Architecture  
<p align="center"> <img src="user Experience.png" alt="System Architecture Diagram" width="800"/> </p>
> The architecture integrates **gesture recognition**, **speech recognition**, **text-to-gloss translation**, and **AI motion synthesis** into one seamless skeleton-based pipeline.

**Flow Summary:**
1. ğŸ¥ Input Sources (Camera, Microphone, Text, Video)
2. ğŸ§  AI Processing (Gesture Recognition, Speech-to-Text, NLP)
3. ğŸ”¡ Translation Engine (Seq2Seq Transformer for Text â†’ Gloss)
4. ğŸ¦´ Motion Generator (Gloss â†’ Pose Sequence using GAN/Transformer)
5. ğŸ“Š Visualization (2D/3D Skeleton Animation via MediaPipe or Matplotlib)
6. ğŸŒ Web Interface built with React & Tailwind CSS  

---

## ğŸ§  Methodology  

1. **Data Preprocessing** â€“ Extract 3D body and hand landmarks using **MediaPipe** or **OpenPose** from sign language datasets.  
2. **Model Training** â€“ Train **Transformers / GANs (Pose2Sign, SignGAN)** for **gloss-to-pose sequence generation**.  
3. **Text-to-Gloss Mapping** â€“ Use **Seq2Seq models** (Transformer or BERT) to adapt spoken grammar to sign gloss.  
4. **Skeleton Motion Visualization** â€“ Display AI-generated motion using **Matplotlib**, **Three.js**, or **MediaPipe rendering**.  

> The system bypasses the need for avatars or pre-recorded videos by directly generating motion trajectories for 2D/3D skeletons.

---

## ğŸ§° Tools & Frameworks  

| Category | Tools Used |
|-----------|-------------|
| Gesture Recognition | MediaPipe, OpenCV, OpenPose |
| AI Models | TensorFlow, PyTorch, Transformers, SignGAN, Pose2Sign |
| Speech Recognition | Whisper, Wav2Vec2.0, Google Speech-to-Text |
| Visualization | Matplotlib, Three.js, WebGL (skeleton rendering) |
| Front-End | React, Tailwind CSS |
| Back-End | Python, Flask/Django (optional) |

---

## ğŸ§¾ Datasets  

### ğŸ“˜ Arabic Sign Language (ArSL)
- [Arabic Sign Language Alphabet Dataset (Kaggle)](https://www.kaggle.com/datasets/birafaneimane/arabic-sign-language-alphabet-arsl-dataset)
- [Arabic Sign Language Data](https://www.kaggle.com/datasets/mohamedmostafa23334/arabic-sign-language-data)
- [Augmented ArSL Dataset](https://www.kaggle.com/datasets/sabribelmadoui/arabic-sign-language-augmented-dataset)

### ğŸ“— American Sign Language (ASL)
- [MS-ASL: Microsoft Large Scale Dataset](https://microsoft.github.io/data-for-society/dataset?d=MS-ASL-American-Sign-Language-Dataset)
- [ASL Dataset (Kaggle)](https://www.kaggle.com/datasets/ayuraj/asl-dataset)

### ğŸ“™ Pose Extraction Datasets
- **PHOENIX-2014T** (German Sign Language, sentence-level with gloss and pose data)
- **YouTube-ASL** (ongoing dataset for real conversational signs)

---

## ğŸ§­ User Journey  

1. **ğŸ  Home Screen** â€“ Choose between *Live Translate*, *Video Upload*, or *Settings*.  
2. **ğŸ¥ Live Mode** â€“ Sign in front of the camera â†’ Real-time text/voice translation using pose detection.  
3. **ğŸ™ Voice Mode** â€“ Speak into the microphone â†’ Watch generated skeleton motion representing signs.  
4. **ğŸ“¹ Upload Mode** â€“ Upload a video â†’ Extracts audio/text and visualizes corresponding skeleton signing.  
5. **âš™ï¸ Accessibility Panel** â€“ Customize skeleton color, motion speed, and feedback settings.  

---

## ğŸ”® Future Work  

- ğŸ§¬ Integrate **text-to-pose models** (Text2Pose, SignFlow) for smoother skeleton motion.  
- ğŸ§  Expand to **sentence-level translation** using PHOENIX-2014T.  
- ğŸ” Enable **cross-sign translation** (e.g., ASL â†” BSL).  
- ğŸ•¶ï¸ Add **AR-based visualization** for skeleton signs in mixed reality.  
- ğŸ“ˆ Explore **self-supervised pose generation** using multimodal datasets.  

---

## ğŸ§± Project Vision  

> â€œTo create a scalable, inclusive, and multilingual accessibility platform that uses **AI-driven skeleton motion synthesis** to eliminate communication barriers between Deaf and hearing individuals â€” advancing research in sign language translation and human motion generation.â€

---

## ğŸ“œ License  

This project is licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details.  
